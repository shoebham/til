### Category postgres

---

 - [A Better Null Display Character](#a-better-null-display-character)
 - [Add Foreign Key Constraint Without A Full Lock](#add-foreign-key-constraint-without-a-full-lock)
 - [Add ON DELETE CASCADE To Foreign Key Constraint](#add-on-delete-cascade-to-foreign-key-constraint)
 - [Adding Composite Uniqueness Constraints](#adding-composite-uniqueness-constraints)
 - [Aggregate A Column Into An Array](#aggregate-a-column-into-an-array)
 - [Assumed Radius Of The Earth](#assumed-radius-of-the-earth)
 - [Auto Expanded Display](#auto-expanded-display)
 - [Between Symmetric](#between-symmetric)
 - [Capitalize All The Words](#capitalize-all-the-words)
 - [Change The Current Directory For psql](#change-the-current-directory-for-psql)
 - [Check If The Local Server Is Running](#check-if-the-local-server-is-running)
 - [Check Table For Any Orphaned Records](#check-table-for-any-orphaned-records)
 - [Checking Inequality](#checking-inequality)
 - [Checking The Type Of A Value](#checking-the-type-of-a-value)
 - [Clear The Screen In psql](#clear-the-screen-in-psql)
 - [Clear The Screen In psql (2)](#clear-the-screen-in-psql-(2))
 - [Compute Hashes With pgcrypto](#compute-hashes-with-pgcrypto)
 - [Compute The Levenshtein Distance Of Two Strings](#compute-the-levenshtein-distance-of-two-strings)
 - [Compute The md5 Hash Of A String](#compute-the-md5-hash-of-a-string)
 - [Configure The Timezone](#configure-the-timezone)
 - [Constructing A Range Of Dates](#constructing-a-range-of-dates)
 - [Convert A String To A Timestamp](#convert-a-string-to-a-timestamp)
 - [Count How Many Records There Are Of Each Type](#count-how-many-records-there-are-of-each-type)
 - [Count Records By Type](#count-records-by-type)
 - [Count The Number Of Trues In An Aggregate Query](#count-the-number-of-trues-in-an-aggregate-query)
 - [Create A Composite Primary Key](#create-a-composite-primary-key)
 - [Create An Index Without Locking The Table](#create-an-index-without-locking-the-table)
 - [Create Database Uses Template1](#create-database-uses-template1)
 - [Create hstore From Two Arrays](#create-hstore-from-two-arrays)
 - [Create Table Adds A Data Type](#create-table-adds-a-data-type)
 - [Creating Conditional Constraints](#creating-conditional-constraints)
 - [Creating Custom Types](#creating-custom-types)
 - [Day Of Week By Name For A Date](#day-of-week-by-name-for-a-date)
 - [Day Of Week For A Date](#day-of-week-for-a-date)
 - [Default Schema](#default-schema)
 - [Defining Arrays](#defining-arrays)
 - [Determine Types Of JSONB Records](#determine-types-of-jsonb-records)
 - [Determining The Age Of Things](#determining-the-age-of-things)
 - [Difference Between Explain And Explain Analyze](#difference-between-explain-and-explain-analyze)
 - [Dump All Databases To A SQL File](#dump-all-databases-to-a-sql-file)
 - [Dump And Restore A Database](#dump-and-restore-a-database)
 - [Duplicate A Local Database](#duplicate-a-local-database)
 - [Edit Existing Functions](#edit-existing-functions)
 - [Enable Logging Of Database Activity](#enable-logging-of-database-activity)
 - [Escaping A Quote In A String](#escaping-a-quote-in-a-string)
 - [Escaping String Literals With Dollar Quoting](#escaping-string-literals-with-dollar-quoting)
 - [Export Query Results To A CSV](#export-query-results-to-a-csv)
 - [Extracting Nested JSON Data](#extracting-nested-json-data)
 - [Find Duplicate Records In Table Without Unique Id](#find-duplicate-records-in-table-without-unique-id)
 - [Find Records That Contain Duplicate Values](#find-records-that-contain-duplicate-values)
 - [Find Records That Have Multiple Associated Records](#find-records-that-have-multiple-associated-records)
 - [Find The Data Directory](#find-the-data-directory)
 - [Find The Location Of Postgres Config Files](#find-the-location-of-postgres-config-files)
 - [Fizzbuzz With Common Table Expressions](#fizzbuzz-with-common-table-expressions)
 - [Force SSL When Making A psql Connection](#force-ssl-when-making-a-psql-connection)
 - [Generate A UUID](#generate-a-uuid)
 - [Generate Random UUIDs Without An Extension](#generate-random-uuids-without-an-extension)
 - [Generate Series Of Numbers](#generate-series-of-numbers)
 - [Generating UUIDs With pgcrypto](#generating-uuids-with-pgcrypto)
 - [Get A Quick Approximate Count Of A Table](#get-a-quick-approximate-count-of-a-table)
 - [Get The Size On Disk Of An Index](#get-the-size-on-disk-of-an-index)
 - [Get The Size Of A Database](#get-the-size-of-a-database)
 - [Get The Size Of A Table](#get-the-size-of-a-table)
 - [Get The Size Of An Index](#get-the-size-of-an-index)
 - [Getting A Slice Of An Array](#getting-a-slice-of-an-array)
 - [Group By The Result Of A Function Call](#group-by-the-result-of-a-function-call)
 - [Include All Queries In The Log File](#include-all-queries-in-the-log-file)
 - [Insert A Bunch Of Records With Generate Series](#insert-a-bunch-of-records-with-generate-series)
 - [Insert Just The Defaults](#insert-just-the-defaults)
 - [Install Postgres With uuid-ossp Using asdf](#install-postgres-with-uuid-ossp-using-asdf)
 - [Integers In Postgres](#integers-in-postgres)
 - [Intervals Of Time By Week](#intervals-of-time-by-week)
 - [Is It Null Or Not Null?](#is-it-null-or-not-null?)
 - [Limit Execution Time Of Statements](#limit-execution-time-of-statements)
 - [List All Columns Of A Specific Type](#list-all-columns-of-a-specific-type)
 - [List All Rows In A Table](#list-all-rows-in-a-table)
 - [List All The Databases](#list-all-the-databases)
 - [List All Versions Of A Function](#list-all-versions-of-a-function)
 - [List Available Schemas](#list-available-schemas)
 - [List Connections To A Database](#list-connections-to-a-database)
 - [List Databases Available For Connecting](#list-databases-available-for-connecting)
 - [List Database Objects With Disk Usage](#list-database-objects-with-disk-usage)
 - [List Database Users](#list-database-users)
 - [List Various Kinds Of Objects](#list-various-kinds-of-objects)
 - [Lower Is Faster Than ilike](#lower-is-faster-than-ilike)
 - [Max Identifier Length Is 63 Bytes](#max-identifier-length-is-63-bytes)
 - [Open Heroku Database In Postico From Terminal](#open-heroku-database-in-postico-from-terminal)
 - [pg Prefix Is Reserved For System Schemas](#pg-prefix-is-reserved-for-system-schemas)
 - [Prepare, Execute, and Deallocate Statements](#prepare,-execute,-and-deallocate-statements)
 - [Pretty Print Data Sizes](#pretty-print-data-sizes)
 - [Pretty Printing JSONB Rows](#pretty-printing-jsonb-rows)
 - [Prevent A Query From Running Too Long](#prevent-a-query-from-running-too-long)
 - [Print The Query Buffer In psql](#print-the-query-buffer-in-psql)
 - [Remove Not Null Constraint From A Column](#remove-not-null-constraint-from-a-column)
 - [Renaming A Sequence](#renaming-a-sequence)
 - [Renaming A Table](#renaming-a-table)
 - [Restart A Sequence](#restart-a-sequence)
 - [Restarting Sequences When Truncating Tables](#restarting-sequences-when-truncating-tables)
 - [Salt And Hash A Password With pgcrypto](#salt-and-hash-a-password-with-pgcrypto)
 - [Send A Command To psql](#send-a-command-to-psql)
 - [Set Inclusion With hstore](#set-inclusion-with-hstore)
 - [Set A Seed For The Random Number Generator](#set-a-seed-for-the-random-number-generator)
 - [Set A Statement Timeout Threshold For A Session](#set-a-statement-timeout-threshold-for-a-session)
 - [Sets With The Values Command](#sets-with-the-values-command)
 - [Shorthand Absolute Value Operator](#shorthand-absolute-value-operator)
 - [Show All Versions Of An Operator](#show-all-versions-of-an-operator)
 - [Sleeping](#sleeping)
 - [Special Math Operators](#special-math-operators)
 - [Storing Emails With citext](#storing-emails-with-citext)
 - [String Contains Another String](#string-contains-another-string)
 - [Switch Non-Castable Column Type With Using Clause](#switch-non-castable-column-type-with-using-clause)
 - [Switch The Running Postgres Server Version](#switch-the-running-postgres-server-version)
 - [Temporarily Disable Triggers](#temporarily-disable-triggers)
 - [Temporary Tables](#temporary-tables)
 - [Terminating A Connection](#terminating-a-connection)
 - [The nullif Function](#the-nullif-function)
 - [Timestamp Functions](#timestamp-functions)
 - [Toggling The Pager In PSQL](#toggling-the-pager-in-psql)
 - [Track psql History Separately Per Database](#track-psql-history-separately-per-database)
 - [Truncate All Rows](#truncate-all-rows)
 - [Truncate Tables With Dependents](#truncate-tables-with-dependents)
 - [Turn Timing On](#turn-timing-on)
 - [Two Ways To Compute Factorial](#two-ways-to-compute-factorial)
 - [Two Ways To Escape A Quote In A String](#two-ways-to-escape-a-quote-in-a-string)
 - [Types By Category](#types-by-category)
 - [Union All Rows Including Duplicates](#union-all-rows-including-duplicates)
 - [Use A psqlrc File For Common Settings](#use-a-psqlrc-file-for-common-settings)
 - [Use Argument Indexes](#use-argument-indexes)
 - [Use Not Valid To Immediately Enforce A Constraint](#use-not-valid-to-immediately-enforce-a-constraint)
 - [Using Expressions In Indexes](#using-expressions-in-indexes)
 - [Using Intervals To Offset Time](#using-intervals-to-offset-time)
 - [Who Is The Current User](#who-is-the-current-user)
 - [Word Count for a Column](#word-count-for-a-column)
 - [Write A Query Result To File](#write-a-query-result-to-file)

---

# A Better Null Display Character

By default, `psql` will display null values with whitespace. This makes it
difficult to quickly identify null values when they appear amongst a bunch
of other data. You can pick a better display value for null characters with
`\pset null`. My preference is the following:

```
\pset null 'Ø'
```

I have this in my `.psqlrc` file so that it is used by default every time.

# Add Foreign Key Constraint Without A Full Lock

Adding a foreign key constraint to a large production table can cause a full
table lock resulting in downtime. This is because the entire table needs to be
scanned to check that the constraint is valid.

The amount of locking, and ultimately the impact on your app, can be reduced by
spreading this action across two commands. First is to add the constraint
without checking that all the existing records are valid.

```sql
alter table books
  add constraint fk_books_authors
  foreign key (author_id)
  references authors(id)
  not valid;
```

The constraint will be added immediately and any subsequent inserts or updates
will be subject to the new foreign key constraint.

The second step is to make this constraint valid for all the existing rows.

```sql
alter table books validate constraint fk_books_authors;
```

This "validation acquires only a SHARE UPDATE EXCLUSIVE lock on the table being
altered." This is lower impact than a full table lock.

[Source](https://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-NOTES)

# Add ON DELETE CASCADE To Foreign Key Constraint

The `alter table` command lets you do quite a bit. But when it comes to
altering existing constraints, there is not much you can do. If you want to
add an `on delete cascade` to an existing foreign key constraint, you are
going to need two statements.

The first statement will drop the constraint and the second statement will
recreate it with the addition of the `on delete` clause. Furthermore, you'll
want to do this in a transaction to ensure the integrity of your data during
the transition between indexes.

Here is an example:

```sql
begin;

alter table orders
drop constraint orders_customer_id_fkey;

alter table orders
add constraint orders_customer_id_fkey
foreign key (customer_id)
references customers (id)
on delete cascade;

commit;
```

[source](http://stackoverflow.com/questions/10356484/how-to-add-on-delete-cascade-constraints)

# Adding Composite Uniqueness Constraints

There are two ways in Postgres to create a composite uniqueness constraint;
that is, a constraint that ensures that the combination of two or more
values on a table only appear once. For the following two code snippets,
assume that we have a table relating Pokemon and Trainers and that our
domain restricts each Trainer to only having at most one of each Pokemon.

The first approach is to create a `constraint` directly on the table:

```sql
alter table pokemons_trainers
  add constraint pokemons_trainers_pokemon_id_trainer_id_key
  unique (pokemon_id, trainer_id);
```

The second approach is to create a unique index:

```sql
create unique index pokemons_trainers_pokemon_id_trainer_id_idx
  on pokemons_trainers (pokemon_id, trainer_id);
```

# Aggregate A Column Into An Array

PostgreSQL's `array_agg` function can be used to aggregate a column into an
array. Consider the following column:

```sql
> select num from generate_series(1,5) as num;
 num
-----
   1
   2
   3
   4
   5
```

By wrapping the `array_agg` aggregate function around `num` we are able to
*aggregate* the values in that column into an array, like so:

```sql
> select array_agg(num) from generate_series(1,5) as num;
  array_agg
-------------
 {1,2,3,4,5}
```

See the docs on [aggregate
functions](http://www.postgresql.org/docs/current/static/functions-aggregate.html)
for more details.

# Assumed Radius Of The Earth

Using the
[`earthdistance`](https://www.postgresql.org/docs/8.3/static/earthdistance.html)
module, we can get the assumed radius of the earth (in meters).

```sql
> create extension cube;
CREATE EXTENSION

> create extension earthdistance;
CREATE EXTENSION

> select earth();
  earth
---------
 6378168
```

# Auto Expanded Display

By default, postgres has expanded display turned off. This means that
results of a query are displayed *horizontally*.
At times, the results of a query can be so wide that line wrapping occurs.
This can make the results and their corresponding column names rather
difficult to read. In these situations, it is preferable to turn on expanded
display so that results are displayed *vertically*.
The `\x` command can be used to toggle expanded display on and off.

Having to toggle expanded display on and off depending on the way a
particular set of results is going to display can be a bit tedious.
Fortunately, running `\x auto` will turn on auto expanded display. This
means postgres will display the results normally when they fit and only
switch to expanded display when it is necessary.

h/t Jack Christensen

# Between Symmetric

PostgreSQL's `between` construct allows you to make a comparison _between_
two values (numbers, timestamps, etc.).

```sql
> select * from generate_series(1,10) as numbers(a) where numbers.a between 3 and 6;
 a
---
 3
 4
 5
 6
```

If you supply an empty range by using the larger of the two values first, an
empty set will result.

```sql
> select * from generate_series(1,10) as numbers(a) where numbers.a between 6 and 3;
 a
---
```

Tacking `symmetric` onto the `between` construct is one way to avoid this
issue.

```sql
> select * from generate_series(1,10) as numbers(a) where numbers.a between symmetric 6 and 3;
 a
---
 3
 4
 5
 6
```

> BETWEEN SYMMETRIC is the same as BETWEEN except there is no requirement
> that the argument to the left of AND be less than or equal to the argument
> on the right. If it is not, those two arguments are automatically swapped,
> so that a nonempty range is always implied.

# Capitalize All The Words

PostgreSQL provides the string function `initcap()` as a way of capitalizing
all words. In the process, it cleans up the casing of the remaining parts of
the words.

Here are some examples of how it works.

```sql
> select initcap('hello, world');
   initcap
--------------
 Hello, World

> select initcap('HELLO, WORLD');
   initcap
--------------
 Hello, World
```

See the [String Functions and Operators
docs](https://www.postgresql.org/docs/current/static/functions-string.html)
for more details.

# Change The Current Directory For psql

When you start a `psql` session, your current directory is what `psql` will
use as its current directory. This is important for meta-commands that use
relative paths based on the current directory -- for instance, the `\i`
meta-command for importing files.

You can change the current directory within a `psql` session using the `\cd`
meta-command.

If my current directory is `home` and there is a `sql` directory in `home,
these commands will do the following:

```sql
\! pwd
-- /home
\cd sql
\! pwd
-- /home/sql
```

The `\cd` meta-command even supports tab completion relative to the current
directory.

You can also change to your home directory using just `\cd`.

# Check If The Local Server Is Running

An install of PostgreSQL comes with a number of utilities including the
`pg_isready` command. This command can be used to check if the local PostgreSQL
server is up, running, and ready to receive connections.

If the server has not yet been started, running the command will result in a
`no response` response.

```bash
$ pg_isready
localhost:5432 - no response
```

In this case, the `pg_ctl` command can be used to start the server.

```bash
$ pg_ctl -D $HOME/.asdf/installs/postgres/12.3/data start
waiting for server to start....

...

 done
server started
```

It tells us that the server is started and we can confirm that by again running
`pg_isready`.

```bash
$ pg_isready
localhost:5432 - accepting connections
```

This command is most useful as part of a script, such as in a CI environment.
In that case, you may not want it writing to `stdout`, you just want to use the
command's exit code. For that, you can tack on the `--quiet` flag.

```
$ pg_isready --quiet
```

[source](https://www.postgresql.org/docs/current/app-pg-isready.html)

# Check Table For Any Orphaned Records

If you don't have a foreign key constraint in place to enforce the relationship
between records in two different tables, then there are a number of ways you
could end up with orphaned records. Orphaned records are records that have a
value in an `*_id` column when that value doesn't correspond to any record in
the related table.

For example, let's say we have an `authors` table with an `id` column and a
`books` table with an `author_id` column. If there is a book record with an
`author_id` value that doesn't resolve to any record in the `authors` table,
then that book is an orphaned record.

You can find out if a table has orphaned records like so:

```sql
select count(*)
  from books
  left join authors
    on books.author_id = authors.id
  where authors.id is null
    and books.author_id is not null;
```

We select from our table with the foreign key (`books`) and _left join_ it
against the related table (`authors`). If there are any book records where the
joined author row is `null`, then that book is orphaned.

# Checking Inequality

In most languages there is a `!=` operator for checking inequality of two
things.

Postgres also supports the synonymous `<>` operator for checking inequality.

```sql
> select 1 <> 1;
 ?column?
----------
 f

> select true <> false;
 ?column?
----------
 t

> select 'taco' <> 'burrito';
 ?column?
----------
 t
```

h/t Brian Dunn

[source](https://www.postgresql.org/docs/9.5/static/functions-comparison.html)

# Checking The Type Of A Value

The `pg_typeof()` function allows you to determine the data type of anything
in Postgres.

```sql
> select pg_typeof(1);
 pg_typeof
-----------
 integer
(1 row)

> select pg_typeof(true);
 pg_typeof
-----------
 boolean
(1 row)
```

If you try it on an arbitrary string, it is unable to disambiguate which
string type (e.g. `text` vs `varchar`).

```sql
> select pg_typeof('hello');
 pg_typeof
-----------
 unknown
(1 row)
```

You just have to be a bit more specific.

```sql
> select pg_typeof('hello'::varchar);
     pg_typeof
-------------------
 character varying
(1 row)
```

[source](http://www.postgresql.org/docs/9.3/static/functions-info.html#FUNCTIONS-INFO-CATALOG-TABLE)

# Clear The Screen In psql

The `psql` interactive terminal does not have a built-in way of clearing the
screen. What I usually do if I really need the screen cleared is quit, run
`clear` from the shell, and then open a new `psql` session. This is
unnecessary though. Instead, we can use the `\!` command to execute a shell
command, in this case, the `clear` command.

```
> \! clear
```

This clears the screen in one step and keeps our current session running.

See the [psql
docs](http://www.postgresql.org/docs/current/static/app-psql.html) for more
details.

# Clear The Screen In psql (2)

In [Clear The Screen In psql](clear-the-screen-in-psql.md), I showed how you
can shell out to the `clear` command as a way of clearing the screen in
`psql`.

It turns out there is an even simpler way. Just hit `CTRL-l`.

# Compute Hashes With pgcrypto

The `pgcrypto` extension that comes with PostgreSQL adds access to some
general hashing functions. Included are `md5`, `sha1`, `sha224`, `sha256`,
`sha384` and `sha512`. Any of these hashing functions can be applied to an
arbitrary string using the `digest` function. Here are example of the `md5`
and `sha1` algorithms:

```sql
> create extension pgcrypto;
CREATE EXTENSION

> select digest('Hello, World!', 'md5');
               digest
------------------------------------
 \x65a8e27d8879283831b664bd8b7f0ad4

> select digest('Hello, World!', 'sha1');
                   digest
--------------------------------------------
 \x0a0a9f2a6772942557ab5355d76af442f8f65e01
```

See the [`pgcrypto` docs](
http://www.postgresql.org/docs/current/static/pgcrypto.html) for more
details.

# Compute The Levenshtein Distance Of Two Strings

PostgreSQL has a built-in function for computing the [Levenshtein
distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two
strings.

```sql
> select levenshtein('hello', 'world');
 levenshtein
-------------
           4

> select levenshtein('function', 'funtcion');
 levenshtein
-------------
           2
```

Check out the [`fuzzystrmatch`
module](https://www.postgresql.org/docs/current/fuzzystrmatch.html#id-1.11.7.24.6)
for more details.

# Compute The md5 Hash Of A String

One of the functions provided by PostgreSQL for working with string data is
the `md5()` function. This function calculates the md5 hash of a given string.

It works like this:

```sql
> select md5('Hello, World!');
               md5
----------------------------------
 65a8e27d8879283831b664bd8b7f0ad4

> select md5('$3cr3tp4$$w0rd');
               md5
----------------------------------
 bbabecfd4031211077473734bae7249f
```

There are more hashing algorithms provided by the `pgcrypto` extension. See
[Compute Hashes With pgcrypto](postgres/compute-hashes-with-pgcrypto.md) for
more details on that.

# Configure The Timezone

Running `show timezone;` will reveal the timezone for your postgres
connection. If you want to change the timezone for the duration of the
connection, you can run something like

```
> set timezone='America/New_York';
SET
> show timezone;
 TimeZone
------------------
 America/New_York
(1 row)
```

Now, if you run a command such as `select now();`, the time will be in
Eastern time.

h/t Jack Christensen

# Constructing A Range Of Dates

PostgreSQL offers a number of range types including the `daterange` type.
This can be constructed using the `daterange()` function with two strings
representing the lower and upper bounds of the date range respectively.

```sql
> select daterange('2015-1-1','2015-1-5');
        daterange
-------------------------
 [2015-01-01,2015-01-05)
```

The lower bound is inclusive -- indicated by the `[` character -- and the
upper bound is exclusive -- indicated by the `)` character.

[source](http://www.postgresql.org/docs/current/static/rangetypes.html)

# Convert A String To A Timestamp

If you have a string that represents a point in time, there are a couple
ways that you can convert it to a PostgreSQL `timestamptz` value.

If the string is in [ISO 8601
format](https://en.wikipedia.org/wiki/ISO_8601), then it can be simply cast
to `timestamptz`.

```sql
> select '2018-10-24'::timestamptz;
      timestamptz
------------------------
 2018-10-24 00:00:00-05
```

A more general purpose approach is to use the
[`to_timestamp`](https://www.postgresql.org/docs/11/static/functions-formatting.html)
function.

```sql
> select to_timestamp('2018-10-24', 'YYYY-MM-DD');
      to_timestamp
------------------------
 2018-10-24 00:00:00-05
```

The first argument is our string-to-be-converted in whatever format. The
second argument is another string describing in what format that string is.

Note: Both of these approaches produce a `timestamptz` value.

# Count How Many Records There Are Of Each Type

Let's say I have a `books` table full of data. One of the columns on this table
is `status` which represents whether the book is published, in review, or still
a draft.

We can find out how many records (books) there are for each `status` using a
`group by` clause and the `count` aggregate function.

```sql
> select status, count(*)
    from books
    group by status;

  status   | count
-----------+-------
 ø         |   123
 published |   611
 draft     |   364
 review    |   239
(4 rows)
```

Because we don't have a `not null` constraint on the `status` column, there are
also some records that have a null value.

We can take this a step further by ordering the output in a consistent
way—descending order of the count column.

```sql
> select status, count(*)
    from books
    group by status
    order by 2 desc;

  status   | count
-----------+-------
 published |   611
 draft     |   364
 review    |   239
 ø         |   123
(4 rows)
```

This `order by` clauses uses [a positional index from the select
arguments](use-argument-indexes.md), so the `2` references the `count(*)`
argument.

# Count Records By Type

If you have a table with some sort of type column on it, you can come up
with a count of the records in that table by type. You just need to take
advantage of `group by`:

```sql
> select type, count(*) from pokemon group by type;

  type   | count 
-----------------
 fire    |    10
 water   |     4
 plant   |     7
 psychic |     3
 rock    |    12
```

# Count The Number Of Trues In An Aggregate Query

The `sum` function is an aggregate function that allows you to sum up a bunch
of integers. What if you want to sum up a boolean column? You may want to know
how many times `true` appears in a collection of grouped records.

This can be done by mixing in a `case` statement.

```sql
select
  author_id,
  sum(case when available then 1 else 0 end)
from books
group by author_id;
```

Here, we are able to find out for each author how many books they have
available.

If we want to count `false` values, we can just invert the `sum` statement:

```sql
sum(case when available then 0 else 1 end)
```

[source](https://stackoverflow.com/a/5396728/535590)

# Create A Composite Primary Key

The unique identifier for a given row in a table is the *primary key*.
Generally, a row can be uniquely identified by a single data point (such as
an id), so the primary key is simply that single data point. In some cases,
your data can be more appropriately uniquely identified by multiple values.
This is where composite primary keys can lend a hand. Consider an example
`plane_tickets` table where each ticket can be uniquely identified by the
passenger and flight it is associated with:

```sql
create table plane_tickets (
  passenger_id integer references passengers not null,
  flight_id integer references flights not null,
  confirmation_number varchar(6) not null,
  seat_assignment varchar not null,
  primary key (passenger_id, flight_id)
);
```

# Create An Index Without Locking The Table

When creating an index for a column, the process of building the index will
lock the column's table. For small datasets this isn't a concern because the
index will take no time at all to create. For larger datasets, the lock could
last long enough to create meaningful downtime. This can all be avoided by
telling Postgres to build the index concurrently.

```sql
create index concurrently idx_book_isbns on books(isbn);
```

Creating the index this way will take a bit longer and put more strain on
machine resources, but it allows concurrent inserts, updates, or deletes on the
table. In other words, you can add an index to a large table in a production
environment without bringing down your app.

Read more about the [details and potential
caveats](https://www.postgresql.org/docs/current/sql-createindex.html#SQL-CREATEINDEX-CONCURRENTLY)
in the docs.

# Create Database Uses Template1

Whenever you use the [`create
database`](https://www.postgresql.org/docs/current/sql-createdatabase.html)
query, unless otherwise specified, it will create it by cloning `template1` by
default.

You can view, inspect, and even modify this database template by connecting to
it.

```sql
\c template1
```

Every Postgres cluster starts with two templates.

```sql
select datname from pg_database where datistemplate = true;
  datname
-----------
 template1
 template0
(2 rows)
```

You cannot however connect to and modify `template0`. It is a fallback clone of
`template1` that you can utilize if you ever modify `template1` and need to
restore it.

[source](https://supabase.io/blog/2020/07/09/postgresql-templates/)

# Create hstore From Two Arrays

PostgreSQL allows the use of the `hstore` type by enabling the `hstore`
extension. One way to create an instance of `hstore` data is by passing two
arrays to the `hstore()` function. The first array is a set of keys and the
second array is the set of values.

```sql
> select hstore(array['one','two','three'], array['1','2','3']);
                hstore
--------------------------------------
 "one"=>"1", "two"=>"2", "three"=>"3"
```

The two arrays must be the same length or an error will occur.

```sql
> select hstore(array['one','two','three'], array['1','2']);
ERROR:  arrays must have same bounds
```

# Create Table Adds A Data Type

Each time you create a table in PostgreSQL, a new data type represented by
that table is created and added to the `pg_type` table. According to the
Postgres docs:

> CREATE TABLE also automatically creates a data type that represents the
> composite type corresponding to one row of the table. Therefore, tables
> cannot have the same name as any existing data type in the same schema.

For instance, if you create a `users` table like so:

```sql
create table users (
  id serial primary key,
  first_name varchar not null,
  last_name varchar not null
);
```

then the `pg_type` will now contain an entry with a `typname` of `users`.

```sql
select * from pg_type where typname = 'users';
-[ RECORD 1 ]--+------------
typname        | users
typnamespace   | 2200
typowner       | 16384
...
```

h/t Bruce Momjian

# Creating Conditional Constraints

There are times when it doesn't make sense for a constraint to apply to all
records in a table. For instance, if we have a table of pokemon, we may only
want to apply a unique index constraint to the names of non-wild pokemon.
This can be achieved with the following conditional constraint:

```sql
create unique index pokemons_names on pokemons (names)
where wild = false;
```

If we try to insert a non-wild pokemon with a duplicate name, we will get an
error. Likewise, if we try to update a pokemon with a duplicate name from
wild to non-wild, we will get an error.

[source](http://www.postgresguide.com/performance/conditional.html)

# Creating Custom Types

PostgreSQL has support for creating custom types. When you need something
more expressive than the built-in types and you don't want your data spread
across multiple columns, you can instead create a custom type.

```sql
create type dimensions as (
  width integer,
  height integer,
  depth integer
);
```

This new type can then be used in the definition of a new table

```sql
create table moving_boxes (
  id serial primary key,
  dims dimensions not null
);
```

and when inserting data

```sql
insert into moving_boxes (dims) values (row(3,4,5)::dimensions);
```

See the [`create type`
docs](http://www.postgresql.org/docs/current/static/sql-createtype.html) for
more details.

# Day Of Week By Name For A Date

In [Day Of Week For A Date](day-of-week-for-a-date.md), I explained how to
determine what day of the week a date is as an integer with PostgreSQL. This
used the `date_part()` function. By using the `to_char()` function with a
date or timestamp, we can determine the day of the week by name (e.g.
Monday). For instance, to determine what day today is, try a statement like
the following:

```sql
> select to_char(now(), 'Day');
  to_char
-----------
 Sunday
```

The `Day` part of the second argument is just one of many template patterns
that can be used for formatting dates and times.

See [Data Type Formatting
Functions](http://www.postgresql.org/docs/current/static/functions-formatting.html)
in the Postgres docs for more details.

# Day Of Week For A Date

Given a `date` in PostgreSQL

```sql
> select '2050-1-1'::date;
    date
------------
 2050-01-01
```

you can determine the day of the week for that date with the `date_part()`
function

```sql
> select date_part('dow', '2050-1-1'::date);
 date_part
-----------
         6
```

The days of week are `0` through `6`, `0` being Sunday and `6` being
Saturday.

[source](http://www.postgresql.org/docs/current/static/functions-datetime.html)

# Default Schema

Schemas can be used to organize tables within a database. You can see all
the schemas your database has like so

```sql
> select schema_name from information_schema.schemata;
    schema_name
--------------------
 pg_toast
 pg_temp_1
 pg_toast_temp_1
 pg_catalog
 public
 information_schema
(6 rows)
```

When you create a new table, it will need to be placed under one of these
schemas. So if you have a `create table posts (...)`, how does postgres know
what schema to put it under?

Postgres checks your `search_path` for a default.

```sql
> show search_path;
   search_path
-----------------
 "$user", public
(1 row)
```

From our first select statement, we see that there is no schema with my user
name, so postgres uses public as the default schema.

If we set the search path to something that won't resolve to a schema name,
postgres will complain

```sql
> set search_path = '$user';
SET
> create table posts (...);
ERROR:  no schema has been selected to create in
```

# Defining Arrays

In postgres, an array can be defined using the `array` syntax like so:

```sql
> select array['a','b','c'];
  array
---------
 {a,b,c}
```

If you are inserting into an existing array column, you can use the array
literal syntax.

```sql
> create temp table favorite_numbers(numbers integer[]);
CREATE TABLE
> insert into favorite_numbers values( '{7,3,9}' );
INSERT 0 1
> select numbers[2] from favorite_numbers;
 numbers
---------
       3
```

Postgres also supports two-dimensional arrays.

```sql
select array[[1,2,3],[4,5,6],[7,8,9]] telephone;
         telephone
---------------------------
 {{1,2,3},{4,5,6},{7,8,9}}
```

# Determine Types Of JSONB Records

You can stick several different things into a [JSONB postgres
column](https://www.postgresql.org/docs/9.4/datatype-json.html).

> Possible types are object, array, string, number, boolean, and null.

If you are trying to audit what is in them, you might reach for:

```sql
> select pg_typeof(my_jsonb_column) from my_table;
```

That is just gonna spit out `jsonb` over and over, like, I already know that.

What you really want to know is, is the top-level thing an _object_, an
_array_, or maybe just a _string_ or _number_. There are specific JSON
processing functions for this, `json_typeof` and `jsonb_typeof` which you can
call like so:

```sql
> select jsonb_typeof(my_jsonb_column) from my_table;
 jsonb_typeof
--------------
 object
 array
 ...
```

[source](https://www.postgresql.org/docs/9.5/functions-json.html)

# Determining The Age Of Things

In PostgreSQL, we can determine the age of something (or someone) by passing
a timestamp to the `age` function.

For instance, if we want to know how long it has been since y2k, we can run
the following query:

```sql
> select age(timestamp '2000-01-01');
           age
-------------------------
 16 years 4 mons 12 days
```

Additionally, if we want to know the amount of time between two dates, we
can pass two timestamps to the `age` function.

For example, we can find out how old Prince lived to be by passing in the
date of death and then date of birth:

```sql
> select age(timestamp 'April 21, 2016', timestamp 'June 7, 1958');
           age
--------------------------
 57 years 10 mons 14 days
```

h/t Josh Davey

# Difference Between Explain And Explain Analyze

The `explain` statement allows you to gain some insight into the performance of
a query. You may hear `explain` and `explain analyze` referred to
interchangeably in conversation. Though they can both be used to explore how a
query will perform, it's important to know a key difference. `explain analyze`
executes the query, `explain` does not.

For `select` queries, the distinction may not feel that important. For
`insert`s, `update`s, and `delete`s, you'll want to be clear about which one
you are using.

```sql
> explain insert into books (title, author) values ('Fledgling', 'Octavia Butler');
                     QUERY PLAN
----------------------------------------------------
 Insert on books  (cost=0.00..0.01 rows=1 width=76)
   ->  Result  (cost=0.00..0.01 rows=1 width=76)

> select count(*) from books;
 count
-------
     0
```

With `explain`, you get cost estimates of the `insert` statement.

```sql
> explain analyze insert into books (title, author) values ('Fledgling', 'Octavia Butler');
                                          QUERY PLAN
----------------------------------------------------------------------------------------------
 Insert on books  (cost=0.00..0.01 rows=1 width=76) (actual time=0.285..0.285 rows=0 loops=1)
   ->  Result  (cost=0.00..0.01 rows=1 width=76) (actual time=0.012..0.012 rows=1 loops=1)
 Planning time: 0.021 ms
 Execution time: 0.309 ms

> select count(*) from books;
 count
-------
     1
```

With `explain analyze`, you get estimates and actual numbers. You also get a
row inserted in the `books` table.

# Dump All Databases To A SQL File

I recently needed to reinstall my local Postgres installation. I had several
databases with data in that cluster that I wanted to preserve. Before I could
go uninstalling and re-installing Postgres, I needed to dump the entire cluster
of databases.

The `pg_dumpall` command that installs with Postgres can be used for this.

```bash
$ pg_dumpall > postgres_13_1_cluster_dump.sql
```

The command outputs to stdout a SQL dump of all the databases stored in the
data directory of this Postgres instance.

I took this a step further and ignored the `template0` and `template1`
directories because I knew those would come with the new install. I did that by
adding the `--exclude-database` flag with a pattern.

```bash
$ pg_dumpall \
  --exclude-database="template*" \
  > postgres_13_1_cluster_dump.sql
```

This data dump can be restored with the new install using:

```bash
$ psql -f postgres_13_1_cluster_dump.sql postgres
```

I wrote more about this process in [Reinstall Postgres with OpenSSL Using
asdf](https://dev.to/jbranchaud/reinstall-postgresql-with-openssl-using-asdf-cmj).

Also, see `pg_dumpall --help` or the [Postgres
docs](https://www.postgresql.org/docs/current/app-pg-dumpall.html) for more
details.

# Dump And Restore A Database

PostgreSQL comes with two command-line utilities for dumping and then
restoring a database -- `pg_dump` and `pg_restore`, respectively.

Using the `pg_dump` with the `-Fc` flag will create a dump of the given
database in a custom format. The output of this command can be redirected
into a file (the `.dump` extension is a standard convention):

```bash
$ pg_dump -Fc my_database > my_database.dump
```

Using the custom format option provides a couple benefits. The output is
significantly compressed in comparison to a generic SQL dump. The dump and
restoration is more flexible. Lastly, the dump can be performed in parallel
if your machine has multiple cores to work with. Likewise, the restoration
can be done in parallel with multiple jobs.

To restore the dump, create a fresh database and then use `pg_restore`:

```bash
$ createdb my_new_database
$ pg_restore -d my_new_database my_database.dump
```

Note: the dumped tables will depend on some user role. You will need to
ensure that this role exists on the database cluster where the restore is
happening. You can use the `createuser` command if necessary.

See the
[`pg_dump` docs](http://www.postgresql.org/docs/current/static/app-pgdump.html)
and [`pg_restore`
docs](http://www.postgresql.org/docs/current/static/app-pgrestore.html)
for more details.

# Duplicate A Local Database

You can quickly create a new database instance that is a duplicate of another
database. If the existing database is local, you don't need to dump and
restore. Instead you can use the `createdb` command that comes with Postgres:

```bash
$ createdb -O ownername -T originaldb newdb
```

This creates a new database called `newdb` using `originaldb` as a template
(`-T`). This will include the entire schema and data of the original database.
The `-O` flag allows you to specify the owner of the database. Since this is
local, you probably want your primary unix user as the owner.

[source](https://stackoverflow.com/a/6739995/535590)

# Edit Existing Functions

In the `psql` console, use `\ef` with the name of a function to fetch and
open the definition of the function. The function will be opened in your
system `$EDITOR` in the form of a `create or replace function` query.

Executing

```sql
> \ef now
```

will open the following in your default editor

```sql
CREATE OR REPLACE FUNCTION pg_catalog.now()
 RETURNS timestamp with time zone
 LANGUAGE internal
 STABLE STRICT
AS $function$now$function$
```

# Enable Logging Of Database Activity

For logging to be enabled for a PostgreSQL server, it needs to be properly
configured. This means ensuring the `logging_collector` option is on. By
default I believe it is `off`.

This is configured in the `postgresql.conf` file and requires a server restart.

First, to find where the conf file is. I can answer that question in a `psql`
session.

```sql
> show config_file;
                             config_file
---------------------------------------------------------------------
 /Users/jbranchaud/.asdf/installs/postgres/12.3/data/postgresql.conf
(1 row)
```

Now, I can open up that file and search for the line that has
`logging_collector`. I uncomment that line and change `off` to `on`.

```
# This is used when logging to stderr:
logging_collector = on 		# Enable capturing of stderr and csvlog
					# into log files. Required to be on for
					# csvlogs.
					# (change requires restart)
```

This requires a restart of the Postgres server.

```bash
$ ~/.asdf/installs/postgres/12.3/bin/pg_ctl -D ~/.asdf/installs/postgres/12.3/data restart
waiting for server to shut down.... done
server stopped
waiting for server to start...
 done
server started
```

I can now adjust any further logging-related configurations on a server or
session basis. And then view those logs.

# Escaping A Quote In A String

In PostgreSQL, string (`varchar` and `text`) literals are declared with
single quotes (`'`). That means that any string containing a single quote
will need some escaping. The way to escape a single quote is with another
single quote.

```sql
> select 'what''s up!';
  ?column?
------------
 what's up!
```

[source](http://jonathansacramento.com/posts/20160122-improve-postgresql-workflow-vim-dbext.html)

# Escaping String Literals With Dollar Quoting

String literals in PostgreSQL are defined by surrounding the content with
the `'` character. For string literals that contain the `'` character, you
may have seen it escaped with a preceding `'`.

```sql
> select 'Isn''t this nice?';
     ?column?
------------------
 Isn't this nice?
```

This is easy enough to do, but can be error prone and doesn't work well if
SQL is being programmatically generated. A great workaround is to escape
string literals using what is called dollar quoting.

```sql
> select $$Isn't this even nicer?$$;
        ?column?
------------------------
 Isn't this even nicer?
```

Just wrap both ends in `$$` instead of `'`.

[source](https://www.postgresql.org/docs/current/static/sql-syntax-lexical.html)

# Export Query Results To A CSV

Digging through the results of queries in Postgres's `psql` is great if you
are a programmer, but eventually someone without the skills or access may
need to check out that data. Exporting the results of a query to CSV is a
friendly way to share said results because most people will have a program
on their computer that can read a CSV file.

For example, exporting all your pokemon to `/tmp/pokemon_dump.csv` can be
accomplished with:

```sql
copy (select * from pokemons) to '/tmp/pokemon_dump.csv' csv;
```

Because we are grabbing the entire table, we can just specify the table name
instead of using a subquery:


```sql
copy pokemons to '/tmp/pokemon_dump.csv' csv;
```

Include the column names as headers to the CSV file with the `header`
keyword:

```sql
copy (select * from pokemons) to '/tmp/pokemon_dump.csv' csv header;
```

If your user has limited access, you can use the \copy command like so:

```sql
\copy (select * from pokemons) to '/tmp/pokemon_dump.csv' with csv header;
```

[source](http://stackoverflow.com/questions/1120109/export-postgres-table-to-csv-file-with-headings)

# Extracting Nested JSON Data

If you are storing nested JSON data in a postgres JSON column, you are
likely going to find yourself in a situation where you need to access some
of those nested values in your database code. For instance, you may need to
get at the license number in this JSON column

```sql
  owner
--------------------------------------------------------------------------------
'{ "name": "Jason Borne", "license": { "number": "T1234F5G6", "state": "MA" } }'
```

Unfortunately, the `->` operator isn't going to do the trick. You need the
`json_extract_path` function

```sql
> select json_extract_path(owner, 'license', 'number') from some_table;

 json_extract_path
-------------------
   'T1234F5G6'
```

Read more about [JSON Functions and
Operators](http://www.postgresql.org/docs/9.4/static/functions-json.html).

# Find Duplicate Records In Table Without Unique Id

I recently came across a couple methods for listing out instances of duplicate
records in a table where the table doesn't have an explicit unique identifier.
Here is [a post](find-records-that-contain-duplicate-values.md) that explains
how to do this when a unique identifier is present.

If the table doesn't have an explicit primary key or other uniquely identifying
value, then we'll have to get some help from [PostgreSQL's internal system
columns](https://www.postgresql.org/docs/current/ddl-system-columns.html) —
namely the `ctid`.

The `ctid` is:

> The physical location of the row version within its table.

Let's use the example of the `mailing_list` table with potential duplicate
`email` values.

Here is the [first approach](https://stackoverflow.com/a/26773018/535590):

```sql
delete from mailing_list
where ctid not in (
  select min(ctid)
  from mailing_list
  group by email
);
```

This uses a subquery to find the first occurrence of every unique email and
then deletes the rest. The `ctid` is the unique value that we can call the
`min` aggregate on.

A [second approach](https://stackoverflow.com/a/46775289/535590):

```sql
delete from mailing_list ml1
  using mailing_list ml2
where ml1.ctid < ml2.ctid
  and ml1.email = ml2.email;
```

This uses `delete using` to join the table against itself as a cartesian
product to compare every entry to every other entry.

# Find Records That Contain Duplicate Values

Let's say I have a `mailing_list` table that contains all the email addresses
that I want to send a mailing out to. Without a uniqueness constraint on the
`email` column, I can end up with multiple records containing the same email
address — duplicates.

Here are a couple queries for checking to see if any duplicate records exist
and which ones they are.

```sql
select email
from (
  select
    email,
    row_number() over (
      partition by email
      order by email
    ) as row_num
  from mailing_list
) t
where t.row_num > 1;
```

This is cool because it uses a [window
function](https://www.postgresql.org/docs/current/tutorial-window.html),
specifically the
[`row_number()`](https://www.postgresql.org/docs/current/functions-window.html)
window function, to assign an incrementing number to each row in the partition.

Here is another, conceptually simpler approach.

```sql
select
  email
  count(*)
from mailing_list
group by email
having count(*) > 1
order by email;
```

Though we cannot use a `where` clause with an aggregate (`count`), we can reach
for a `having` clause to grab only those results where we've found more than
`1` — duplicates.

[source](https://www.postgresqltutorial.com/how-to-delete-duplicate-rows-in-postgresql/)

# Find Records That Have Multiple Associated Records

A common type of table association in a relational database is a one-to-many
relationship. For instance, a database representing a bookshelf may have an
`authors` table where each record can be associated with multiple records in
the `books` table. That relationship is represented by a `author_id` foreign
key column on `books` that points to `authors.id`.

We can write a query to find all authors that have not zero or one, but
multiple books by doing a join and then tacking on a `having` clause.

```sql
select authors.id, authors.name, count(books.id)
  from authors
  join books
    on authors.id = books.author_id
  group by authors.id
  having count(books.id) >= 2;
```

This will result in a listing of author ids, author names, and their number of
books.

It does this by joining books to authors, grouping by the `authors.id` to
produce a set of records unique to each author, and then combining multiple
books by aggregating them with a `count`. The `having` clause is necessary
because it is our way of _filtering_ on an aggregate value, in this case the
`count`.

# Find The Data Directory

Where does postgres store all of the data for a database cluster? Well, in
its data directory. Where exactly that data directory is can depend on how
the database cluster was setup. Postgres can tell you right where to look,
though. Within `psql`, run

```sql
> show data_directory
```

Postgres will output the absolute path of the data directory.

[source](http://dba.stackexchange.com/questions/1350/how-do-i-find-postgresqls-data-directory)

# Find The Location Of Postgres Config Files

If you can connect to your database with `psql`, then you can easily find
the location of your Postgres config files. After connecting, I can ask
Postgres to show me where the main config file is:

```sql
> show config_file;
                                  config_file
--------------------------------------------------------------------------------
 /Users/jbranchaud/Library/Application Support/Postgres/var-9.5/postgresql.conf
```

In the same directory as that `postgresql.conf` file are a number of other
configuration files such as the `pg_hba.conf` file.

h/t Dillon Hafer

# Fizzbuzz With Common Table Expressions

In learning about CTEs (common table expressions) in postgres, I discovered
that you can do some interesting and powerful things using the `with
recursive` construct. The following solves the fizzbuzz problem for integers
up to 100

```sql
with recursive fizzbuzz (num,val) as (
    select 0, ''
    union
    select (num + 1),
      case
      when (num + 1) % 15 = 0 then 'fizzbuzz'
      when (num + 1) % 5  = 0 then 'buzz'
      when (num + 1) % 3  = 0 then 'fizz'
      else (num + 1)::text
      end
    from fizzbuzz
    where num < 100
)
select val from fizzbuzz where num > 0;
```

Check out [With Queries (Common Table Expressions)](http://www.postgresql.org/docs/9.4/static/queries-with.html)
for more details on CTEs.

# Force SSL When Making A psql Connection

If you try connecting `psql` to a Postgres server that requires SSL
connections, you'll see an error message like this:

```
psql: error: FATAL:  SSL/TLS required
```

You can tell `psql` to make an SSL connection by adding the `sslmode=require`
key-value pair to the connection.

If you're using a connection string, that might look like this:

```bash
$ psql postgres://username:password@host.com/dbname?sslmode=require
```

Or if you're using the connection parameter flags, it may look like this, using
the `--set` flag:

```bash
$ PGPASSWORD=password psql \
    --set=sslmode=require \
    -h host.com \
    -U username \
    dbname
```

If after adding `sslmode=require`, you find that SSL support is not compiled
in, [follow the instructions in this post to reinstall Postgres with SSL
support](https://dev.to/jbranchaud/reinstall-postgresql-with-openssl-using-asdf-cmj).

# Generate A UUID

Postgres has support for universally unique identifiers (UUIDs) as a column
data type via `uuid`. If you have a UUID column, you may need to generate a
UUID.  This requires the `uuid-ossp` module. This module provides a number
of functions for generating UUIDs including the `uuid_generate_v4()`
function which bases the UUID entirely off random numbers.

```sql
> create extension "uuid-ossp";
CREATE EXTENSION
> select uuid_generate_v4();
           uuid_generate_v4
--------------------------------------
 a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11
```

See the [postgres docs](http://www.postgresql.org/docs/9.4/static/uuid-ossp.html) for more
details on UUID generation functions.

# Generate Random UUIDs Without An Extension

In other posts I've covered how to generate v4 random UUIDs in PostgreSQL
[using the `uuid-ossp` extension](generate-a-uuid.md) as well as the more
up-to-date method of [using the `pgcrypto`
extension](generating-uuids-with-pgcrypto.md).

As of PostgreSQL v13, you no longer need to add an extension for v4 UUID
generation. It comes built-in as the `gen_random_uuid()` function.

```sql
> select gen_random_uuid();
           gen_random_uuid
--------------------------------------
 0aa72fe6-ede7-4ccf-b328-348becc58066
(1 row)
```

If you need other non-v4 UUID functions, you'll have to stick with
[uuid-ossp](https://www.postgresql.org/docs/current/uuid-ossp.html).

# Generate Series Of Numbers

Postgres has a `generate_series` function that can be used to, well,
generate a series of something. The simplest way to use it is by giving it
`start` and `stop` arguments

```sql
> select generate_series(1,5);
 generate_series 
-----------------
               1
               2
               3
               4
               5
```

The default step is 1, so if you want to count backwards, you need to
specify a negative step


```sql
> select generate_series(5,1,-1);
 generate_series 
-----------------
               5
               4
               3
               2
               1
```

You can use a larger step value to, for instance, get only multiples of 3

```sql
> select generate_series(3,17,3);
 generate_series 
-----------------
               3
               6
               9
              12
              15
```

Trying this out with timestamps is left as an exercise for the reader.

# Generating UUIDs With pgcrypto

If you check out the docs for the [`uuid-ossp`
extension](https://www.postgresql.org/docs/current/static/uuid-ossp.html),
you'll come across the following message.

> The OSSP UUID library... is not well maintained, and is becoming
> increasingly difficult to port to newer platforms. 

A little bit later, it says:

> If you only need randomly-generated (version 4) UUIDs, consider using the
> gen_random_uuid() function from the pgcrypto module instead.

So, if we are using the UUID data type and only need to generate random
UUIDs, we can rely on the [`pgcrypto`
extension](https://www.postgresql.org/docs/current/static/pgcrypto.html). It
comes with the `gen_random_uuid()` function which generates random v4 UUIDs.

```sql
> create extension "pgcrypto";
CREATE EXTENSION

> select gen_random_uuid();
           gen_random_uuid
--------------------------------------
 0a557c31-0632-4d3e-a349-e0adefb66a69

> select gen_random_uuid();
           gen_random_uuid
--------------------------------------
 83cdd678-8198-4d56-935d-d052f2e9db37
```

# Get A Quick Approximate Count Of A Table

Really large PostgreSQL tables can be slow to work with. Even a count of the
rows in a really large table can take a while to tabulate. I'm talking about
tables on the order of hundreds of millions of rows.

For instance, here is a query grabbing the count of a ~400 million row table.

```sql
> select count(*) from events;

   count
-----------
 427462316
(1 row)

Time: 55113.794 ms
```

If I'm willing to wait nearly a minute (55 seconds), I can get an accurate
count of the rows in this `events` table.

If I don't want to wait and an approximate count will do, there are faster
ways. One way is to query the `pg_class` table.

```
> select reltuples::numeric as count
  from pg_class
  where relname='events';

   count
-----------
 427462000
(1 row)

Time: 0.413 ms
```

The resulting count is within hundreds of the actual value and tells me what I
need to know. And instead of 55 seconds, it takes less than half a millisecond.

[source](https://andyatkinson.com/postgresql-tips)

# Get The Size On Disk Of An Index

Indexes, when added to the right columns, can provide massive performance gains
for certain queries. Indexes aren't free though. It is worth noting that they
take up disk space. The amount of disk space they take is generally irrelevant,
but it is at least worth being aware of. Especially if you're deal with a
massive table.

You can check the current size of an index on disk with [PostgreSQL's
`pg_relation_size`
function](https://www.postgresql.org/docs/current/functions-admin.html).

First, you'll want to look up the name of the index you're curious about.
Running `\d table_name` (replacing `table_name` with the name of the table the
index is on) will show you everything about the table including its indexes and
their names.

Then run the following query:

```sql
select pg_size_pretty(pg_relation_size('index_users_on_email'));
 pg_size_pretty
----------------
 41 MB
(1 row)
```

This one is pretty small. They can get pretty big though. I've seen some that
take up over 1GB on disk.

# Get The Size Of A Database

If you have connect access to a PostgreSQL database, you can use the
`pg_database_size()` function to get the size of a database in bytes.

```sql
> select pg_database_size('hr_hotels');
 pg_database_size
------------------
          8249516
```

Just give it the name of the database and it will tell you how much disk
space that database is taking up.

Checkout [the Postgres docs](http://www.postgresql.org/docs/current/static/functions-admin.html) for more details.

# Get The Size Of A Table

In [Get The Size Of A Database](get-the-size-of-a-database.md), I showed a
PostgreSQL administrative function, `pg_database_size()`, that gets the size
of a given database. With the `pg_relation_size()` function, we can get the
size of a given table. For instance, if we'd like to see the size of the
`reservations` table, we can executing the following query:

```sql
> select pg_relation_size('reservations');
 pg_relation_size
------------------
          1531904
```

This gives us the size of the `reservations` table in bytes. As you might
expect, the referenced table needs to be part of the connected database and
on the search path.

See [the Postgres docs](http://www.postgresql.org/docs/current/static/functions-admin.html) for more details.

# Get The Size Of An Index

Want to get an idea of how much disk space that additional index is taking
up? You can query for it with the same methods discussed in [Get The Size Of
A Table](get-the-size-of-a-table.md) and [Pretty Print Data
Sizes](pretty-print-data-sizes.md).

For instance, if I have a table with a `users_pkey` index and a
`users_unique_lower_email_idx` index, I can check the sizes like so:

```sql
> select pg_size_pretty(pg_relation_size('users_pkey'));
 pg_size_pretty
----------------
 240 kB

> select pg_size_pretty(pg_relation_size('users_unique_lower_email_idx'));
 pg_size_pretty
----------------
 704 kB
```

[source](https://www.niwi.nz/2013/02/17/postgresql-database-table-indexes-size/)

# Getting A Slice Of An Array

Postgres has a very natural syntax for grabbing a slice of an array. You
simply add brackets after the array declaring the lower and upper bounds
of the slice separated by a colon.

```sql
> select (array[4,5,6,7,8,9])[2:4];
  array
---------
 {5,6,7}
```

Notice that the bounds are inclusive, the array index is `1`-based, and the
array declaration above needs to be wrapped in parentheses in order to not
trip up the array slice syntax.

You can also select rectangular slices from two dimensional arrays like so:

```sql
> select (array[[1,2,3],[4,5,6],[7,8,9]])[2:3][1:2];
     array
---------------
 {{4,5},{7,8}}
```

# Group By The Result Of A Function Call

Typically, a query that I write involving a `group by` will look more or less
like this:

```sql
select category, count(*)
  from products
  group by category;
```

The `category` column is the thing I'm grouping by. In this case, I'm doing a
little data exploration.

We are not strictly limited to grouping by a column. We can use all sorts of
functions offered by Postgres to get at more interesting results. [String
functions](https://www.postgresql.org/docs/current/functions-string.html) are a
great place to start.

Let's say our `products` table also has an `identifier` column with a naming
scheme where the first three letters of the identifier correspond to the
product's classification. We can group by that part of the `identifier`:

```sql
select substring(identifier from 1 for 3), count(*)
  from products
  group by substring(identifier from 1 for 3);
```

The funkiness of the `substring` syntax aside, we were able to group our
products in a new way and learn something about our data.

# Include All Queries In The Log File

The default log-level (`log_statement` setting) for a PostgreSQL server is
`none`. Other valid log-levels for [that setting are `ddl`, `mod`, and
`all`](https://www.postgresql.org/docs/13/runtime-config-logging.html).

If you want to see all the queries hitting a database, you'll want to set it to
`all`. This can be set as a server-wide setting or it can be set on a
per-session basis.

Because `all` is so noisy, I like to use it on a per-session basis when I'm in
a situation where I know I'd like to see all queries.

```sql
> set log_statement = 'all';
```

After running that statement in my `psql` session, I can tail the log file to
keep an eye on queries hitting the database.

Be sure that [logging is enabled via the
`logging_collector`](enable-logging-of-database-activity.md) as well.

# Insert A Bunch Of Records With Generate Series

Sometimes you want to quickly insert a bunch of fake (or real) data into a
Postgres table. This is a great way to populate seed data or set up data
scenarios for testing things out.

For instance, I recently used the following query to generate a bunch of data
in a `roles` table to test out a query performance issue.

```sql
> insert into roles (
    name,
    resource_id,
    resource_type,
    created_at,
    updated_at
  )
  select
    'organization_user',
    g.id,
    'Organization',
    now(),
    now()
  from generate_series(1,54000) as g(id);
```

The key part is the [`generate_series()`
function](https://www.postgresql.org/docs/current/functions-srf.html) which
will produce a row for every value between the two arguments. In this case, it
will generate `1`, `2`, `3`, etc. all the way up to `54000` -- so 54k rows in
total.

The query selects off that generated series with some static values and some
timestamps to create sufficiently fake data that can be inserted into the
specified columns of the `roles` table.

This quickly inserts tens of thousands of records that I can now use to test
out the performance of a SQL query.

# Insert Just The Defaults

If you are constructing an `INSERT` statement for a table whose required
columns all have default values, you may just want to use the defaults. In
this situation, you can break away from the standard:

```
> insert into table_name (column1, column2) values (value1, value2);
```

Instead, simply tell Postgres that you want it to use the default values:

```
> insert into table_name default values;
```

# Install Postgres With uuid-ossp Using asdf

The `uuid-ossp` extension is part of `postgres-contrib` and is often included
with installs of PostgreSQL. By default, when installing PostgreSQL with
[`asdf`](https://asdf-vm.com/#/) using the
[`asdf-postgres`](https://github.com/smashedtoatoms/asdf-postgres) plugin, the
`uuid-ossp` extension is not included.

Without it I had trouble running schema migrations against a database that was
trying to create the `uuid-ossp` extension:

> postgresql uuid-ossp.control file missing in extention folder

To include `uuid-ossp` when installing Postgres with `asdf`, you'll need to
include _extra config options_.

For instance, to install Postgres 9.6.21 with `uuid-ossp` included:

```bash
$ POSTGRES_EXTRA_CONFIGURE_OPTIONS="--with-uuid=e2fs" asdf install postgres 9.6.21
```

There are some resources that recommend using `--with-uuid=ossp`, but that
appears to require a prerequisite install of a separate package, so I prefer
the `e2fs` option.

[source](https://github.com/smashedtoatoms/asdf-postgres/issues/4#issuecomment-350592132)

# Integers In Postgres

Postgres has three kinds of integers. Or rather three sizes of integers.
There are `smallint` (`int2`), `integer` (`int4`), and `bigint` (`int8`)
integers. As you might expect, they are 2 byte, 4 byte, and 8 byte integers
respectively. They are also signed integers. All of this has implications
for what ranges of integers can be represented by each type.

The `smallint` integers have 2 bytes to use, so they can be used to
represent integers from -32768 to +32767.

The `integer` integers have 4 bytes to use, so they can be used to represent
integers from -2147483648 to +2147483647.

The `bigint` integers have 8 bytes to use, so they can be used to represent
integers from -9223372036854775808 to +9223372036854775807.

Though columns can be restricted to use a particular-sized integer, postgres
is smart enough to default to `integer` and only use `bigint` as necessary
when working with integers on the fly.

```sql
> select pg_typeof(55);
 pg_typeof
-----------
 integer

> select pg_typeof(99999999999999999);
 pg_typeof
-----------
 bigint
```

# Intervals Of Time By Week

It is pretty common to use hours or days when creating a Postgres
interval. However, intervals can also be created in week-sized chunks

```sql
> select '2 weeks'::interval;
 interval
----------
 14 days
(1 row)

> select make_interval(0,0,7,0,0,0,0);
 make_interval
---------------
 49 days
(1 row)
```

# Is It Null Or Not Null?

In PostgreSQL, the standard way to check if something is `NULL` is like so:

```sql
select * as wild_pokemons from pokemons where trainer_id is null;
```

To check if something is not null, you just add `not`:

```sql
select * as captured_pokemons from pokemons where trainer_id is not null;
```

PostgreSQL also comes with `ISNULL` and `NOTNULL` which are non-standard
ways of doing the same as above:


```sql
select * as wild_pokemons from pokemons where trainer_id isnull;
```

```sql
select * as captured_pokemons from pokemons where trainer_id notnull;
```

# Limit Execution Time Of Statements

You can limit the amount of time that postgres will execute a statement
by setting a hard timeout. By default the timeout is 0 (see `show
statement_timeout;`) which means statements will be given as much time as
they need.

If you do want to limit your statements, to say, 1 second, you can set the
execution time like so

```sql
> set statement_timeout = '1s';
SET
> show statement_timeout;
 statement_timeout
-------------------
 1s
(1 row)
```

Any queries taking longer than 1 second will be aborted with the following
message output

```
ERROR:  canceling statement due to statement timeout
```

# List All Columns Of A Specific Type

We can access information about all the columns in our database through the
`information_schema` tables; in particular, the `columns` table. After
connecting to a particular database, we can list all columns (across all our
tables) of a specific type. We just need to know the schema of the tables we
are interested in and the type that we want to track down.

My application's tables are under the `public` schema and I want to track
down all `timestamp` columns. My query can look something like this

```sql
> select table_name, column_name, data_type from information_schema.columns where table_schema = 'public' and data_type = 'timestamp without time zone';
   table_name    | column_name |          data_type
-----------------+-------------+-----------------------------
 articles        | created_at  | timestamp without time zone
 articles        | updated_at  | timestamp without time zone
 users           | created_at  | timestamp without time zone
 users           | updated_at  | timestamp without time zone
(4 rows)
```

Alternatively, I could look for both `timestamp` and `timestamptz` with a
query like this

```sql
> select table_name, column_name, data_type from information_schema.columns where table_schema = 'public' and data_type like '%timestamp%';
```

# List All Rows In A Table

Perhaps the more common way to list all rows in a table is with the
following `select` command:

```sql
select * from bedding_types;
```

There is an alternative approach that also selects all rows from a table.
It's essentially a shorthand -- the `table` command.

```sql
> table bedding_types;
   name
----------
 No Bed
 1 Full
 1 Double
 2 Double
 1 Twin
 2 Twins
 1 Queen
 2 Queen
 1 King
 2 Kings
 3 Kings
 Murphy
 Sofa Bed
```

h/t Jack Christensen

# List All The Databases

There are two ways to list all the available databases. The first is a
`psql` only command:

```
\list
```

The second approach is to query the `pg_database` table. Something like the
following will suffice:

```sql
select datname from pg_database;
```

# List All Versions Of A Function

Within `psql` you can use `\df` to list a postgres function with a given
name

```sql
> \df now
                              List of functions
   Schema   | Name |     Result data type     | Argument data types |  Type
------------+------+--------------------------+---------------------+--------
 pg_catalog | now  | timestamp with time zone |                     | normal
(1 row)
```

When a function has multiple definitions across a number of types, `\df`
will list all versions of that function

```sql
> \df generate_series
List of functions
-[ RECORD 1 ]-------+-------------------------------------------------------------------
Schema              | pg_catalog
Name                | generate_series
Result data type    | SETOF bigint
Argument data types | bigint, bigint
Type                | normal
-[ RECORD 2 ]-------+-------------------------------------------------------------------
Schema              | pg_catalog
Name                | generate_series
Result data type    | SETOF bigint
Argument data types | bigint, bigint, bigint
Type                | normal
-[ RECORD 3 ]-------+-------------------------------------------------------------------
Schema              | pg_catalog
Name                | generate_series
Result data type    | SETOF integer
Argument data types | integer, integer
Type                | normal
-[ RECORD 4 ]-------+-------------------------------------------------------------------
Schema              | pg_catalog
Name                | generate_series
Result data type    | SETOF integer
Argument data types | integer, integer, integer
Type                | normal
-[ RECORD 5 ]-------+-------------------------------------------------------------------
Schema              | pg_catalog
Name                | generate_series
Result data type    | SETOF timestamp with time zone
Argument data types | timestamp with time zone, timestamp with time zone, interval
Type                | normal
-[ RECORD 6 ]-------+-------------------------------------------------------------------
Schema              | pg_catalog
Name                | generate_series
Result data type    | SETOF timestamp without time zone
Argument data types | timestamp without time zone, timestamp without time zone, interval
Type                | normal
```

# List Available Schemas

Use the `\dn` command within a `psql` session to list the available schemas.
This will only included user created schemas. This means that schemas like
`public` will be listed whereas schemas like `information_schema` and
`pg_catalog` will not.

You can use `\dnS` to also list system schemas.

[source](http://www.postgresql.org/docs/current/static/app-psql.html)

# List Connections To A Database

The `pg_stat_activity` table can be used to determine what connections there
currently are to the PostgreSQL server and to a particular database. To see
the process ids and usernames of all connection to your PostgreSQL server,
run the following query:

```sql
> select pid, usename from pg_stat_activity;
  pid  |  usename
-------+------------
 57174 | jbranchaud
 83420 | jbranchaud
```

Include `datname` in the requested columns to figure out the database of
each connection.

```sql
> select pid, usename, datname from pg_stat_activity;
  pid  |  usename   |  datname
-------+------------+-----------
 57174 | jbranchaud | hr_hotels
 83420 | jbranchaud | pgbyex
```

The results can be restricted to a particular database as necessary.

```sql
> select pid, usename from pg_stat_activity where datname = 'hr_hotels';
  pid  |  usename
-------+------------
 57174 | jbranchaud
```

# List Databases Available For Connecting

I tend to have a couple different versions of Postgres installed on my
development machine. Each server version tends to have a different set of
databases. As I switch between projects and Postgres versions, it can be hard
to remember the name of the database to which I want to connect when using
`psql`.

I usually connect to one of the defaults, which is either named `postgres` or
named after the machine user.

There is a better way. I can first ask `psql` to list all the available
databases.

```
❯ psql --list
Timing is on.
                                               List of databases
              Name              |   Owner    | Encoding |   Collate   |    Ctype    |     Access privileges
--------------------------------+------------+----------+-------------+-------------+---------------------------
 jbranchaud                     | jbranchaud | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 postgres                       | jbranchaud | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 thirty_days_server_development | jbranchaud | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 thirty_days_server_test        | jbranchaud | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
(4 rows)
```

Then I know before connecting which one I'm looking for or if it must be in the
data directory of another Postgres server version.

# List Database Objects With Disk Usage

I'll often times use `\d` or `\dt` to check out the tables in my database.
This shows the schema, object name, object type (e.g. `table`), and owner
for each.

By adding the `+` to that meta-command, I can also see the disk usage for
each database object.

Here is an example of look at all tables in a database with the additional
`Size` (or disk usage) information:

```sql
> \dt+
                              List of relations
 Schema |        Name        | Type  |   Owner    |    Size    | Description
--------+--------------------+-------+------------+------------+-------------
 public | amount_types       | table | jbranchaud | 16 kB      |
 public | ingredient_amounts | table | jbranchaud | 8192 bytes |
 public | ingredient_types   | table | jbranchaud | 16 kB      |
 public | ingredients        | table | jbranchaud | 48 kB      |
 public | recipes            | table | jbranchaud | 16 kB      |
 public | schema_migrations  | table | jbranchaud | 16 kB      |
 public | users              | table | jbranchaud | 16 kB      |
```

# List Database Users

Within `psql`, type `\du` to list all the users for a database and their
respective permissions.

```bash
> \du
                              List of roles
 Role name  |                   Attributes                   | Member of
------------+------------------------------------------------+-----------
 jbranchaud | Superuser, Create role, Create DB, Replication | {}
 sampleuser | Create DB                                      | {}
 ```

# List Various Kinds Of Objects

Our PostgreSQL database can end up with all kinds of objects: tables,
sequences, views, etc. We can use a variety of `psql` meta-commands to list
the different types of (user-created) objects in our database.

- `\dt` will list all the tables
- `\dE` will list all the foreign tables
- `\di` will list all the indexes
- `\ds` will list all the sequences
- `\dv` will list all the views
- `\dm` will list all the materialized views

These can also be combined. For instance, to see all the tables and
sequences, we can run `\dts`.

# Lower Is Faster Than ilike

There are a couple ways to do a case-insensitive comparison of data in
PostgreSQL. One way is to use the `ilike` operator for comparison. Another
way is to use the `lower()` function on both sides of the `=` operator for
comparison. Using `lower()` is a bit faster than using `ilike`.

When comparing

```sql
select * from users where email ilike 'some-email@example.com';
```

to

```sql
select * from users where lower(email) = lower('some-email@example.com');
```

we find (via `explain analyze`) that using `lower()` was taking around 12ms
where as the `ilike` example was taking around 17ms.

We earn orders of magnitude in performance when adding a functional index
that uses the `lower()` function like so:

```sql
create unique index users_unique_lower_email_idx on users (lower(email));
```

After adding this index, the example using `lower()` drops to around 0.08ms.

For the full example and `explain analyze` outputs, [see this
document](https://github.com/jbranchaud/postgresing/blob/master/ilike_vs_lower.sql).

# Max Identifier Length Is 63 Bytes

In PostgreSQL, identifiers -- table names, column names, constraint names,
etc. -- are limited to a maximum length of 63 bytes. Identifiers longer than
63 characters can be used, but they will be truncated to the allowed length
of 63.

```sql
> alter table articles
    add constraint this_constraint_is_going_to_be_longer_than_sixty_three_characters_id_idx
    check (char_length(title) > 0);
NOTICE:  identifier "this_constraint_is_going_to_be_longer_than_sixty_three_characters_id_idx" will be truncated to "this_constraint_is_going_to_be_longer_than_sixty_three_characte"
ALTER TABLE
```

Postgres warns us of identifiers longer than 63 characters, informing us of
what they will be truncated to. It then proceeds to create the identifier.

If postgres is trying to generate an identifier for us - say, for a foreign
key constraint - and that identifier is longer than 63 characters, postgres
will truncate the identifier somewhere in the middle so as to maintain the
convention of terminating with, for example, `_fkey`.

The 63 byte limit is not arbitrary. It comes from `NAMEDATALEN - 1`. By default
`NAMEDATALEN` is 64. If need be, this value can be modified in the Postgres
source. Yay, open-source database implementations.

See [the postgres docs](http://www.postgresql.org/docs/current/static/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS) for more details.

# Open Heroku Database In Postico From Terminal

I recently downloaded [Postico](https://eggerapps.at/postico/) at the
recommendation of [Dillon Hafer](https://dillonhafer.com/). I tend to use
`psql` as a PostgreSQL client for all my database querying needs. However,
Dillon highly recommended Postico for doing system admin querying.

I needed to connect directly to a production Postgres server on Heroku to
investigate slow queries. Postico presented me with a form of individual fields
for host, port, username, password, database, etc.

This would have been a little annoying to fill in manually. Dillon had a
shortcut to recommend. From the command line you can open Postico with a
connection string. It knows how to split that connection string into the
respective fields.

```bash
heroku config:get DATABASE_URL --app APP_NAME | xargs open -a Postico
```

This requests the `DATABASE_URL` from Heroku. It is a Postgres connection
string with all the fields needed to connect to a remove server. This is then
passed via `xargs` to `Postico` as it is being `open`ed.

# pg Prefix Is Reserved For System Schemas

Have you ever tried to create a schema with `pg_` as the first part of the
name of the schema? If so, you probably didn't get very far. Postgres won't
let you do that. It reserves the `pg_` prefix for system schemas. If you try
to create a schema in this way, you'll get an *unacceptable schema name*
error.

```sql
> create schema pg_cannot_do_this;
ERROR:  unacceptable schema name "pg_cannot_do_this"
DETAIL:  The prefix "pg_" is reserved for system schemas.
```

# Prepare, Execute, and Deallocate Statements

In PostgreSQL, you can prepare a named statement to be executed later using
[`prepare`](https://www.postgresql.org/docs/current/static/sql-prepare.html).

```sql
> prepare column_names (text) as
    select column_name from information_schema.columns where table_name = $1;
PREPARE
```

These statements are kept around for the duration of the session. To see the
available statements, check out the `pg_prepared_statements` view.

```sql
> select * from pg_prepared_statements;
     name     |                                  statement                                  |         prepare_time          | parameter_types | from_sql
--------------+-----------------------------------------------------------------------------+-------------------------------+-----------------+----------
 column_names | prepare column_names (text) as                                             +| 2017-03-10 15:01:09.154528-06 | {text}          | t
              |   select column_name from information_schema.columns where table_name = $1; |                               |                 |
```

To run a prepared statement, use `execute` with the name of the statement
and any arguments.

```sql
> execute column_names('users');
   column_name
-----------------
 id
 email
 password_digest
 created_at
 updated_at
 first_name
 last_name
```

You can also delete a statement with
[`deallocate`](https://www.postgresql.org/docs/current/static/sql-deallocate.html)
if you'd like.

```sql
> deallocate column_names;
DEALLOCATE
```

# Pretty Print Data Sizes

Use the `pg_size_pretty()` function to pretty print the sizes of data in
PostgreSQL. Given a `bigint`, it will determine the most human-readable
format with which to print the value:

```sql
> select pg_size_pretty(1234::bigint);
 pg_size_pretty
----------------
 1234 bytes

> select pg_size_pretty(123456::bigint);
 pg_size_pretty
----------------
 121 kB

> select pg_size_pretty(1234567899::bigint);
 pg_size_pretty
----------------
 1177 MB

> select pg_size_pretty(12345678999::bigint);
 pg_size_pretty
----------------
 11 GB
```

This function is particularly useful when used with the
[`pg_database_size()`](get-the-size-of-a-database.md) and
[`pg_relation_size()`](get-the-size-of-a-table.md) functions.

```sql
> select pg_size_pretty(pg_database_size('hr_hotels'));
 pg_size_pretty
----------------
 12 MB
```

# Pretty Printing JSONB Rows

Who needs a document store when you can just use PostgreSQL's `JSONB` data
type? Viewing rows of `JSONB` output can be challenging though because it
defaults to printing them as a single line of text.

```sql
> select '{"what": "is this", "nested": {"items 1": "are the best", "items 2": [1, 2, 3]}}'::jsonb;
                                      jsonb
----------------------------------------------------------------------------------
 {"what": "is this", "nested": {"items 1": "are the best", "items 2": [1, 2, 3]}}
(1 row)
```

Fortunately, Postgres comes with a function for prettying up the format of
the output of these rows --
[`jsonb_pretty`](https://www.postgresql.org/docs/current/static/functions-json.html)

```sql
> select jsonb_pretty('{"what": "is this", "nested": {"items 1": "are the best", "items 2": [1, 2, 3]}}'::jsonb);
            jsonb_pretty
------------------------------------
 {                                 +
     "what": "is this",            +
     "nested": {                   +
         "items 1": "are the best",+
         "items 2": [              +
             1,                    +
             2,                    +
             3                     +
         ]                         +
     }                             +
 }
(1 row)
```

h/t Jack Christensen

# Prevent A Query From Running Too Long

A number of different factors can effect how long a query takes to run.
Certainly the size of a table and the complexity of the query play a big role.
Locking can really slow a query down by making it wait to get started. A series
of competing queries that induce table locking can grind things to a halt.

If you don't want queries in a particular connection being allowed to wait or
run too long, you can set a timeout.

```sql
set statement_timeout to '500';
```

That will ensure that any statement run in that connection will be terminated
if it takes longer than 500ms.

You can also specify a unit:

```sql
set statement_timeout to '15s';
```

That will enforce statement timeout of 15 seconds.

See the
[docs](https://www.postgresql.org/docs/current/runtime-config-client.html) for
more details.

# Print The Query Buffer In psql

I'll often be composing a PostgreSQL query in Vim and decide I want to give
it a try in `psql`. I copy the relevant snippet of SQL to my system buffer
and then paste into `psql`. I'm usually hit with a mess of text like this
though:

```sql
jbranchaud=# create table nullable_fields (
jbranchaud(#   id serial primary key,
  first varchar,
  last varchar
)
  id serial primary key,
jbranchaud(#   first varchar,
  last varchar
)
  first varchar,
jbranchaud(#   last varchar
)
  last varchar
jbranchaud(# )
)
jbranchaud-#
```

Yikes. That's not readable. Fortunately, `psql` provides a command for
printing the current contents of the query buffer. By typing `\p` I'll see a
more readable version of what I just pasted in.

```sql
jbranchaud-# \p
create table nullable_fields (
  id serial primary key,
  first varchar,
  last varchar
)
jbranchaud-#
```

After taking another glance at the snippet of SQL, I decide to complete the
query to create my new table.

```sql
jbranchaud-# ;
CREATE TABLE
```

# Remove Not Null Constraint From A Column

When you want to add a [`not
null`](https://www.postgresql.org/docs/current/ddl-constraints.html#id-1.5.4.6.6)
constraint to a column, you do so by _setting_ it.

```sql
alter table books
  alter column publication_date
  set not null;
```

You can remove a `not null` constraint from a column, by _dropping_ it.

```sql
alter table books
  alter column publication_date
  drop not null;
```

Notice this excerpt of syntax from the official Postgres docs:

```
... ALTER [ COLUMN ] column_name { SET | DROP } NOT NULL
```

[source](https://www.postgresql.org/docs/current/sql-altertable.html)

# Renaming A Sequence

If a table is created with a `serial` type column, then a sequence is also
created with a name based on the name of the table.

```sql
> \d
            List of relations
 Schema |       Name      |   Type   |   Owner
--------+-----------------+----------+------------
 public | accounts        | table    | jbranchaud
 public | accounts_id_seq | sequence | jbranchaud
```

In [Renaming A Table](renaming-a-table.md), I showed how a table can be
renamed in PostgreSQL. This will not, however, rename associated sequences.
To maintain naming consistency, you may want to also rename sequences when
renaming tables. This can be done with a query like the following:

```sql
> alter sequence accounts_id_seq rename to users_id_seq;
```

See the [`alter
sequence`](http://www.postgresql.org/docs/current/static/sql-altersequence.html)
docs for more details.

# Renaming A Table

Using the `alter table` command in PostgreSQL, you can rename an existing
table. This command will also update any references to the table such as via
foreign key constraints. Just run a command like the following:

```sql
alter table ingredient_types rename to item_types;
```

Note that this may result in breaking a number of conventions. Foreign keys,
sequences, and constraints with names eponymous to the original table will
no longer follow convention despite the references being updated. These can
be renamed as well if desired.

See
[`renaming_table.sql`](https://github.com/jbranchaud/postgresing/blob/master/renaming/rename_table.sql)
for a full example.

See the [`alter table`
docs](http://www.postgresql.org/docs/current/static/sql-altertable.html) for
more details.

# Restart A Sequence

In postgres, if you are truncating a table or doing some other sort of
destructive action on a table in a development or testing environment, you
may notice that the id sequence for the primary key just keeps plugging
along from where it last left off. The sequence can be reset to any value
like so:

```sql
> alter sequence my_table_id_seq restart with 1;
ALTER SEQUENCE
```

[source](http://www.postgresql.org/docs/current/static/sql-altersequence.html)

# Restarting Sequences When Truncating Tables

PostgreSQL's
[`truncate`](http://www.postgresql.org/docs/current/static/sql-truncate.html)
feature is a handy way to clear out all the data from a table. If you use
`truncate` on a table that has a `serial` primary key, you may notice that
subsequent insertions keep counting up from where you left off. This is
because the sequence the table is using hasn't been restarted. Sure, you can
restart it manually or you can tell `truncate` to do it for you. By
appending `restart identity` to the end of a `truncate` statement, Postgres
will make sure to restart any associated sequences at `1`.

```sql
truncate pokemons, trainers, pokemons_trainers restart identity;
```

# Salt And Hash A Password With pgcrypto

The
[`pgcrypto`](http://www.postgresql.org/docs/current/static/pgcrypto.html)
extension that ships with PostgreSQL can be used to do a number of
interesting things. This includes functions for doing salted password
hashing. Using the `crypt` and `gen_salt` functions, we can securely store a
user password and later compare it to plain-text passwords for
authentication purposes.

```sql
create extension pgcrypto;

select crypt('pa$$w0rd', gen_salt('bf'));
                            crypt
--------------------------------------------------------------
 $2a$06$Z7wmrkYMOyLboLcULUYzNe6nHUcWywSZTt6nSrT5Xdv/VLdJ4g99K

> select (
    '$2a$06$Z7wmrkYMOyLboLcULUYzNe6nHUcWywSZTt6nSrT5Xdv/VLdJ4g99K' =
    crypt(
      'pa$$w0rd',
      '$2a$06$Z7wmrkYMOyLboLcULUYzNe6nHUcWywSZTt6nSrT5Xdv/VLdJ4g99K'
    )
  ) as matched;
 matched
---------
 t

> select (
    '$2a$06$Z7wmrkYMOyLboLcULUYzNe6nHUcWywSZTt6nSrT5Xdv/VLdJ4g99K' =
    crypt(
      'password',
      '$2a$06$Z7wmrkYMOyLboLcULUYzNe6nHUcWywSZTt6nSrT5Xdv/VLdJ4g99K'
    )
  ) as matched;
 matched
---------
 f
```

The salt value is generated using the blowfish encryption algorithm (hence,
the `'bf'`). There is support for other algorithms such as `md5`.

See the
[`pgcrypt` documentation](http://www.postgresql.org/docs/current/static/pgcrypto.html) for
more details.

# Send A Command To psql

You can send a command to `psql` to be executed by using the `-c` flag

```bash
$ psql -c "select 'Hello, World!';"
   ?column?
---------------
 Hello, World!
(1 row)
```

Specify a particular database as needed

```bash
$ psql blog_prod -c 'select count(*) from posts;'
 count 
-------
     8 
(1 row)
```

h/t Jack Christensen

# Set Inclusion With hstore

In PostgreSQL, `hstore` records can be compared via set inclusion. The `@>`
and `<@` operators can be used for this. The `@>` operator checks if the
right operand is a subset of the left operand. The `<@` operator checks if
the left operand is a subset of the right operand.

```sql
> select '"one"=>"1", "two"=>"2", "three"=>"3"'::hstore @> '"two"=>"2"'::hstore;
 ?column?
 ----------
  t

> select '"one"=>"1", "two"=>"2", "three"=>"3"'::hstore <@ '"two"=>"2"'::hstore;
 ?column?
----------
 f
```

See the [`hstore` PostgreSQL
docs](http://www.postgresql.org/docs/current/static/hstore.html) for more
details.

# Set A Seed For The Random Number Generator

In PostgreSQL, the internal seed for the random number generator is a
run-time configuration parameter. This `seed` parameter can be set to a
particular seed in order to get some determinism from functions that utilize
the random number generator. The seed needs to be something between `0` and
`1`.

We can see this in action by setting the seed and then invoking `random()` a
couple times. Doing this twice, we will see the reproducibility we can
achieve with a seed.

```sql
> set seed to 0.1234;
SET

> select random();
      random
-------------------
 0.397731185890734

> select random();
      random
------------------
 0.39575699577108
(1 row)

> set seed to 0.1234;
SET

> select random();
      random
-------------------
 0.397731185890734

> select random();
      random
------------------
 0.39575699577108
```

The seed can also be configured with the `setseed()` function.

See [the PostgreSQL
docs](http://www.postgresql.org/docs/8.3/static/sql-set.html) for more
details.

# Set A Statement Timeout Threshold For A Session

The `statement_timeout` variable is used to tell the PostgreSQL server that you
want it to terminate statements (queries and transactions) that run past the
specified threshold. This is a great way to [prevent runaway
queries](https://blog.crunchydata.com/blog/control-runaway-postgres-queries-with-statement-timeout)
in a production environment.

You can set this threshold with a `set` statement. It can take an integer
argument of milliseconds. Here I set it to a timeout of 1 minute.

```sql
> set statement_timeout = 60000;
SET
> show statement_timeout;
 statement_timeout
-------------------
 1min
(1 row)
```

This will set the `statement_timeout` for the duration of the session. It won't
effect other sessions.

You can also set the threshold with a string argument which allows you to
include a unit of time. Here I set it to 30 seconds.

```sql
> set statement_timeout = '30s';
SET
> show statement_timeout;
 statement_timeout
-------------------
 30s
(1 row)
```

Now that the `statement_timeout` is set to `30s`, I can run a query that I know
will exceed that threshold
([`pg_sleep`](https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-DELAY)).

```sql
> select pg_sleep(31);
ERROR:  canceling statement due to statement timeout
Time: 30001.997 ms (00:30.002)
```

After 30 seconds have passed, the Postgres server will interrupt the query.

# Sets With The Values Command

You can concisely create sets of values in PostgreSQL using the `values`
command.

```sql
> values (1), (2), (3);
 column1
---------
       1
       2
       3
```

You can even create multiple columns of values.

```sql
> values (1, 'a', true), (2, 'b', false);
 column1 | column2 | column3
---------+---------+---------
       1 | a       | t
       2 | b       | f
```

This is most often used with an insert command, but can be used on its own,
as a subquery, within a CTE, etc.

[source](http://www.postgresql.org/docs/current/static/sql-values.html)

# Shorthand Absolute Value Operator

Postgres offers many [math
functions](https://www.postgresql.org/docs/8.0/functions-math.html) including
`abs` for computing the absolute value of a number.

```sql
> select abs(-1);
 abs
-----
   1
(1 row)
```

There is also an absolute value _operator_ -- the `@` symbol. This can be used
to do the same thing.

```sql
> select @ -1;
 ?column?
----------
        1
(1 row)
```

[source](https://kb.objectrocket.com/postgresql/why-use-postgres-abs-function-in-sql-729)

# Show All Versions Of An Operator

We may be familiar with PostgreSQL's containment operator (`@>`). Maybe
we've used it with an array before, so we understand the general idea. But
now we are curious about what are the other types with which this
containment operator can be used.

We can quickly find out the answer with the `\do` command in `psql`:

```sql
> \do @>
                               List of operators
   Schema   | Name | Left arg type | Right arg type | Result type | Description
------------+------+---------------+----------------+-------------+-------------
 pg_catalog | @>   | aclitem[]     | aclitem        | boolean     | contains
 pg_catalog | @>   | anyarray      | anyarray       | boolean     | contains
 pg_catalog | @>   | anyrange      | anyelement     | boolean     | contains
 pg_catalog | @>   | anyrange      | anyrange       | boolean     | contains
 pg_catalog | @>   | box           | box            | boolean     | contains
 pg_catalog | @>   | box           | point          | boolean     | contains
 pg_catalog | @>   | circle        | circle         | boolean     | contains
 pg_catalog | @>   | circle        | point          | boolean     | contains
 pg_catalog | @>   | jsonb         | jsonb          | boolean     | contains
 pg_catalog | @>   | path          | point          | boolean     | contains
 pg_catalog | @>   | polygon       | point          | boolean     | contains
 pg_catalog | @>   | polygon       | polygon        | boolean     | contains
 pg_catalog | @>   | tsquery       | tsquery        | boolean     | contains
```

The `Left arg type` and `Right arg type` columns tell us what we need to
know.

This `\do` command can be used with any operator for a similar set of
information.

h/t Bruce Momjian

# Sleeping

Generally you want your SQL statements to run against your database as
quickly as possible. For those times when you are doing some sort of
debugging or just want your queries to look very computationally expensive,
PostgreSQL offers the `pg_sleep` function.

To sleep for 5 seconds, try the following:

```sql
> select now(); select pg_sleep(5); select now();
              now
-------------------------------
 2016-01-08 16:30:21.251081-06
(1 row)

Time: 0.274 ms
 pg_sleep
----------

(1 row)

Time: 5001.459 ms
              now
-------------------------------
 2016-01-08 16:30:26.252953-06
(1 row)

Time: 0.260 ms
```

As you'll notice, the `pg_sleep` statement took about 5 seconds.

[source](http://www.if-not-true-then-false.com/2010/postgresql-sleep-function-pg_sleep-postgres-delay-execution/)

# Special Math Operators

Postgres has all the mathematical operators you might expect in any
programming language (e.g. `+`,`-`,`*`,`/`,`%`). It also has a few extras
that you might not be expecting.

Factorial Operator:

```sql
> select 5!;
 ?column?
----------
      120
(1 row)
```

Square Root Operator:

```sql
> select |/81;
 ?column?
----------
        9
(1 row)
```

Absolute Value Operator:

```sql
> select @ -23.4;
 ?column?
----------
     23.4
(1 row)
```

# Storing Emails With citext

Email addresses should be treated as case-insensitive because they are. If a
user is trying to sign in with their email address, we shouldn't care if
they type `user@example.com` or `User@example.com`. Both of those email
addresses should be treated as equal and ultimately lead us to the same
`User` record.

With the
[`citext`](http://www.postgresql.org/docs/current/static/citext.html)
extension, we can create a column that acts as a case-insensitive text type.
Any comparisons on a column of that type will internally have the `lower`
function executed on the arguments.

The following example shows this in action:

```sql
create extension if not exists citext;

create table citext_emails (
  id serial primary key,
  email citext not null unique
);

insert into citext_emails (email) values ('LizLemon@nbc.com');

select * from citext_emails where email = 'lizlemon@nbc.com';
--  id |      email
-- ----+------------------
--   1 | LizLemon@nbc.com
```

See
[`citext-emails.sql`](https://github.com/jbranchaud/postgresing/blob/master/citext-emails.sql)
for a full example.

# String Contains Another String

You can check if a string *contains* another string using the `position`
function.

```sql
> select position('One' in 'One Two Three');
 position
----------
        1
```

It returns the 1-based index of the first character of the first match of
that substring.

```sql
> select position('Four' in 'One Two Three');
 position
----------
        0
```

If the substring doesn't appear within the string, then the result is 0.

Thus, you can determine if a string *contains* another string by checking if
the value resulting from `position` is greater than 0.

# Switch Non-Castable Column Type With Using Clause

With certain data types, such as from `int` to `bigint` or `timestamptz` to
`timestamp`, there is an automatic type casting that can take place with
existing data. This means Postgres knows how to handle a data type change
like:

```sql
alter table users
  alter column id
  set data type bigint;
```

With other data types, such as `int` to `uuid`, there is no way for Postgres to
know how to automatically cast it. To change the data type of a column in a
scenario like this, you have to tell Postgres how to handle the conversion with
a `using` clause.

```sql
alter table users
  alter column id
  set data type uuid using (gen_random_uuid());
```

In this instance, the `using` clause tells Postgres to ignore the existing
integer `id` value and use the `gen_random_uuid()` function to generate a UUID
value to take its place.

The `using` clause can also reference the existing column value as part of its
type cast.

See the [alter table
documentation](https://www.postgresql.org/docs/current/sql-altertable.html) for
more details on this.

# Switch The Running Postgres Server Version

I use [asdf](https://github.com/asdf-vm/asdf) install and manage multiple
versions of Postgres on my Mac OSX machine. With `asdf` and project-based
`.tools-versions` files, I can control what version of Postgres (`psql`) I use
at a project-level.

The one snag with this workflow is managing the currently running server
version.  Lets say I need to switch from a project using `12.3` to a project
using `13.1`. If the Postgres server running on my machine is using the
Postgres server 12.3, then I'll need to manually stop that server and start up
the Postgres server 13.1.

This can be done like so:

```bash
# stop the 12.3 server
$ $HOME/.asdf/installs/postgres/12.3/bin/pg_ctl \
    -D $HOME/.asdf/installs/postgres/12.3/data \
    stop

# start the 13.1 server
$ $HOME/.asdf/installs/postgres/13.1/bin/pg_ctl \
    -D $HOME/.asdf/installs/postgres/13.1/data \
    start
```

This uses the specific asdf-versioned `pg_ctl` command to stop and start the
servers.

I've found it tedious to dig up these commands each time I need to switch, so I
added a [`switch_pg` function to my `~/.zshrc`
config](https://gist.github.com/jbranchaud/3cda6be6e1dc69c6f55435a387018dac).

# Temporarily Disable Triggers

In general, you are always going to want your triggers to fire. That's why
they are there. Though special circumstances may arise where you need to
temporarily disable them. Use

```sql
> set session_replication_role = 'replica';
SET
```

By changing the
[replication role](http://www.postgresql.org/docs/9.4/static/runtime-config-client.html#GUC-SESSION-REPLICATION-ROLE)
from `origin` to
`replica` you are essentially disabling all non-replica triggers across the
database (for that session). When you are done, you can simply set the
replication role back so that normal trigger behavior can resume

```sql
> set session_replication_role = 'origin';
SET
```

A more direct and fine-grained approach to disabling triggers is to use an
`alter table` command that targets a specific trigger.

h/t Jack Christensen

# Temporary Tables

Create a temporary table in Postgres like so

```sql
create temp table posts (
    ...
);
```

This table (and its data) will only last for the duration of the session.
It is created on a schema specific to temporary tables. It is also worth
noting that it won't be autovacuumed, so this must be done manually as
necessary.

# Terminating A Connection

Consider the scenario where you are trying to drop a database, but there are
existing connections.

```bash
$ dropdb sample_db
dropdb: database removal failed: ERROR:  database "sample_db" is being accessed by other users
DETAIL:  There is 1 other session using the database.
```

If you don't know where these connections are, you can terminate them within
a `psql` session. You just have to figure out the `pid` of those
connections. In [List Connections To A
Database](list-connections-to-a-database.md), I explained how to
get at the `pid` values of connections. Using the `pid` value and
`pg_terminate_backend()`, you can terminate a connection.

```sql
> select pg_terminate_backend(12345);
 pg_terminate_backend
----------------------
 t
```

To terminate all connections to a particular database, use a query like the
following:

```sql
select pg_terminate_backend(pg_stat_activity.pid)
from pg_stat_activity
where pg_stat_activity.datname = 'sample_db'
  and pid <> pg_backend_pid();
 pg_terminate_backend
----------------------
 t
```

This excludes the current session, so you'll need to exit `psql` as well
before dropping the database.

[source](http://stackoverflow.com/questions/5408156/how-to-drop-a-postgresql-database-if-there-are-active-connections-to-it)

# The nullif Function

PostgreSQL, in addition to generalized case statements, includes the
[`nullif`](https://www.postgresql.org/docs/current/functions-conditional.html)
function. The docs describe it as a way "to perform the inversation operation
of a `coalesce`".

Rather than resolving to some fallback value if the primary value is `null`
(like `coalesce` does), it will resolve to `null` if the given values are the
same.

```sql
> select nullif(0, 0);
 nullif
--------
      ø
(1 row)
```

If the values are not equal, then the first value is the result of the
function.

```sql
> select nullif(1, 0);
 nullif
--------
      1
(1 row)
```

One way this can be used is in conjunction with the `coalesce` function. For
instance, if I have a table of values that are either 0 or a positive number, I
can coerce all the zeros to be `1` like so.

```sql
> select coalesce(nullif(0, 0), 1);
 coalesce
----------
        1
(1 row)
```

h/t [Ian Jones](https://twitter.com/_jonesian)

# Timestamp Functions

There are a handful of timestamp functions available in postgres. The most
common one is probably `now()`. This is an alias of
`transaction_timestamp()` which the postgres docs describe as:

> Current date and time (start of current transaction)

Two other interesting timestamp functions are `statement_timestamp()` and
`clock_timestamp()`. The postgres docs describe `statement_timestamp()` as:

> Current date and time (start of current statement)

Using `statement_timestamp()` throughout a transaction will yield different
results from statement to statement.

The postgres docs describe `clock_timestamp()` as:

> Current date and time (changes during statement execution)

Using `clock_timestamp()` may even yield different results depending on
where it appears in a given statement.

Try running something like this to see:

```postgresql
> select clock_timestamp(), clock_timestamp(), clock_timestamp(), clock_timestamp();
        clock_timestamp        |        clock_timestamp        |        clock_timestamp        |        clock_timestamp        
-------------------------------+-------------------------------+-------------------------------+------------------------------
 2015-03-20 14:58:49.832592-05 | 2015-03-20 14:58:49.832592-05 | 2015-03-20 14:58:49.832593-05 | 2015-03-20 14:58:49.832593-05
```

You'll notice that we see a change in the clock time at the microsecond
level mid-way through the statement.

sources: [postgres docs](http://www.postgresql.org/docs/9.1/static/functions-datetime.html) and
[Jack C.](http://hashrocket.com/team/jack-christensen)

# Toggling The Pager In PSQL

When the pager is enabled in `psql`, commands that produce larger output
will be opened in a pager. The pager can be enabled within `psql` by running
`\pset pager on`.

If you'd like to retain the output of commands, perhaps as reference for
subsequent commands, you can turn the pager off. As you might expect, the
pager can be disabled with `\pset pager off`.

[source](http://stackoverflow.com/questions/11180179/postgresql-disable-more-output)

# Track psql History Separately Per Database

By default, `psql` will keep track of all recent queries and commands in the
`.psql_history` file in your home directory.

When in a `psql` session, you can hit the `Up` key to go back through the
history to find a previously entered query. That means you can quickly retrieve
and rerun past queries.

However the default `psql` configuration means that your history can contain
queries from a `psql` session with another database that don't make sense in
the context of the current database.

You can keep these query histories separate by configuring `psql` to use
separate history files per database. This can be done by adding the following
line to your `~/.psqlrc` file.

```
\set HISTFILE ~/.psql_history-:DBNAME
```

[source](https://github.com/hashrocket/dotmatrix/commit/1bd581db3a7192eb7aaa766a97e4b4b82d544067)

# Truncate All Rows

Given a postgres database, if you want to delete all rows in a table, you
can use the `DELETE` query without any conditions.

```sql
> delete from pokemons;
DELETE 151
```

Though `DELETE` can do the job, if you really are deleting all rows to clear
out a table, you are better off using `TRUNCATE`. A `TRUNCATE` query will be
faster than a `DELETE` query because it will just delete the rows without
scanning them as it goes.

```sql
> truncate pokemons;
TRUNCATE TABLE
```

[source](http://www.postgresql.org/docs/8.2/static/sql-truncate.html)

# Truncate Tables With Dependents

In [Truncate All Rows](truncate-all-rows.md), I talked about how
postgres's `truncate` can be used to quickly delete all rows in a table. In
practice this alone won't be very useful though, because tables usually have
other tables that depend on them via foreign keys. If you have tables `A`
and `B` where `B` has a foreign key referencing `A`, then trying to truncate
`A` will result in something like this:

```sql
> truncate A;
ERROR:  cannot truncate a table referenced in a foreign key constraint
```

Fortunately, `truncate` has some tricks up its sleeve.

If you know two tables are tied together via a foreign key constraint, you
can just truncate both of them at once:

```sql
> truncate A, B;
TRUNCATE TABLE;
```

If many tables are tied together in this way and you are looking to throw
all of it out, then a simpler approach is to cascade the truncation:

```sql
> truncate A cascade;
NOTICE:  truncate cascades to table "B"
TRUNCATE TABLE
```

Use these with care and potentially within transactions because your data
will go bye bye.

h/t Dillon Hafer and Jack Christensen

# Turn Timing On

When digging around your database and running queries, it is helpful to
have an eye on the speed of those queries. This can give insight into
where there are needs for optimizations.

Turn timing on (and off) within `psql` by running `\timing`. With timing
on, the duration of each query will be displayed in milliseconds after the
output of the query.

# Two Ways To Compute Factorial

In PostgreSQL, there are two ways to compute the factorial of a number.
There is a prefix operator and a postfix operator. The prefix operator is
`!!` and can be used like so:

```sql
> select !!5;
 ?column?
----------
      120
```

The postfix operator is `!` and can be used like so:

```sql
> select 5!;
 ?column?
----------
      120
```

See the [mathematical functions and operators
docs](http://www.postgresql.org/docs/8.1/static/functions-math.html)
for more details.

# Two Ways To Escape A Quote In A String

String literals in PostgreSQL have to be wrapped in single quotes. This can be
tricky if you are faced with writing out a query using a string that contains a
single quote.

```sql
> select 'who's on first?';
...
```

The query won't execute because it is waiting for you to close the second set
of quotes.

I know of two ways to handle this situation.

The first is to put two single quotes back to back. The first will cause the
second to be escaped so that the quote shows up in the string.

```sql
> select 'who''s on first?';
    ?column?
-----------------
 who's on first?
(1 row)
```

The second is to prepend the string with [the `E`
character](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-STRINGS)
to allow escape sequences in strings.

```sql
> select E'who\'s on first?';
    ?column?
-----------------
 who's on first?
(1 row)
```

[source](https://stackoverflow.com/a/12320729)

# Types By Category

Postgres has many types, each of which fall into a particular category.
These categories include Array, Boolean, String, Numeric, Composite, etc.
Each of these categories has a corresponding code. For instance, numeric
types have a code of `N`. Using `N` I can get a list of all the numeric
types:

```sql
> select typname from pg_type where typcategory = 'N';
     typname
-----------------
 int8
 int2
 int4
 regproc
 oid
 float4
 float8
 money
 numeric
 regprocedure
 regoper
 regoperator
 regclass
 regtype
 regconfig
 regdictionary
 cardinal_number
(17 rows)
```

Check out
[`pg_type`](http://www.postgresql.org/docs/current/interactive/catalog-pg-type.html)
in the Postgres docs for a list of all categories and codes.

# Union All Rows Including Duplicates

Two tables or sets of results can be joined together into a single result set
using [the `union`
operator](https://www.postgresql.org/docs/current/queries-union.html). When
combining results with `union`, all duplicate rows will be removed from its
result.

```sql
> select generate_series(1,4)
  union
  select generate_series(3,6)
  order by 1 asc;

 generate_series
-----------------
               1
               2
               3
               4
               5
               6
(6 rows)
```

Notice that despite both sides of the `union` having their own 3 and 4, those
values each only show up once in the result.

If we don't want duplicates to be excluded, we can use `union all`.

```sql
> select generate_series(1,4)
  union all
  select generate_series(3,6)
  order by 1 asc;

 generate_series
-----------------
               1
               2
               3
               3
               4
               4
               5
               6
(8 rows)
```

In this case we have 8 rows instead of 6 with the values 3 and 4 each appearing
twice.

[source](https://www.postgresqltutorial.com/postgresql-union/)

# Use A psqlrc File For Common Settings

There are a handful of settings that I inevitably turn on or configure each
time I open up a `psql` session. I can save myself a little time and sanity
by configuring these things in a `.psqlrc` dotfile that is located in my
home directory. This will ensure my `psql` session is configured just how I
like it each time I launch it. Here is what my `~/.psqlrc` file currently
looks like:

```
\x auto
\timing
\pset null 'Ø'
```

# Use Argument Indexes

In Postgres, each of the arguments you specify in a `select` statement has a
1-based index tied to it. You can use these indexes in the `order by` and
`group by` parts of the statement.

Instead of writing

```sql
select id, updated_at from posts order by updated_at;
```

you can write

```sql
select id, updated_at from posts order by 2;
```

If you want to group by a table's `type` and then order by the counts from
highest to lowest, you can do the following

```sql
select type, count(*) from transaction group by 1 order by 2 desc;
```

# Use Not Valid To Immediately Enforce A Constraint

When adding a constraint to a table, you can optionally include `not valid`.
This tells Postgres that it doesn't need to enforce the constraint on
existing records in the table. At least not immediately. This constraint
will be enforced for any updates and subsequent insertions. Thus, you can
immediately enforce the constraint while giving yourself time to clean up
or massage any existing records that conflict with the constraint.

Here is an example of how you would add a constraint this way:

```sql
alter table boxes
add constraint check_valid_length
check (length > 0) not valid;
```

Eventually, you will want to ensure that all data in the table conforms to the
constraint. Once you get to that point, you can mark the constraint as valid
with a `validate constraint` command:

```sql
alter table boxes
validate constraint check_valid_length;
```

As long as all records are valid with respect to this constraint, it will be
marked as valid.

h/t Chris Erin

# Using Expressions In Indexes

Though we usually see column names by themselves when defining an index, it
is also possible to create an index with an expression.

Let's say I have a `users` table with an `email` column. Then I may end up
creating an index like this

```sql
create index email_idx on users (email);
```

If I always perform queries on the `email` column with the `lower()`
function, like this

```sql
select * from users where lower(email) = lower('some@email.com');
```

then I will want to also create an index with that full expression --
`lower(email)`

I can do this with a statement like the following

```sql
create index lower_email_idx on users (lower(email));
```

Without an index that uses the full `lower(email)` expression, `select`
statements like the one above will be forced to do full sequential scans
instead of indexed scans.

# Using Intervals To Offset Time

Postgres Intervals can be used with time as a way of determining a
standard offset. For instance, I can concisely determine what the time was 2
hours earlier with

```sql
> select now() - '2 hours'::interval as earlier;
            earlier
-------------------------------
 2015-06-12 21:17:43.678822-05
```

or similarly

```sql
> select now() - interval '2 hours' as earlier;
            earlier
-------------------------------
 2015-06-12 21:17:43.678822-05
```

# Who Is The Current User

You can determine the current user of a psql session by selecting on the `current_user`

```sql
> select current_user;

  current_user
----------------
   test_user
```

You can also select on the `user` which is an alias of `current_user`

```sql
> select user;

     user
----------------
   test_user
```

# Word Count for a Column

Assuming I have a database with a posts table:

```sql
> select * from posts where id = 1;
 id |  title   |              content               
----+----------+------------------------------------
  1 | My Title | This is the content of my article. 
```

I can compute the word count of the content of a given post like so:

```sql
> select sum(array_length(regexp_split_to_array(content, '\s+'), 1)) from posts where id = 1;
 sum 
-----
   7 
```

[source](http://blog.lingohub.com/2013/07/sql-word-count-character-count-postgres/)

# Write A Query Result To File

Generally when writing a query in `psql` a statement will be terminated with
a semicolon. An alternative approach is to end it with a `\g` instead. This
will also send the query to the Postgres server for execution.

```sql
select 1 \g
```

If a filename is included after the `\g`, then the result of the query will
be written to that file instead of output to the `psql` session.

```sql
> select 1, 2, 3 \g query_result.txt
```

If we `cat` that file, we can see the query result.

```sql
Time: 4.293 ms
> \! cat query_result.txt
 ?column? | ?column? | ?column?
----------+----------+----------
        1 |        2 |        3
(1 row)
```

See `man psql` for more details.

